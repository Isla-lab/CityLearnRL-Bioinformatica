To-Do List per il progetto CityLearnRL-Bioinformatica
1. Debug e tuning degli algoritmi (TD3 in cluster, SAC e PPO localmente)
TD3 (esecuzione su cluster):
Verificare che l’ambiente di esecuzione sul cluster sia configurato correttamente (versione di Python, CityLearn e librerie richieste come specificato nell'env del progetto).
Eseguire il codice TD3 sul cluster assicurandosi che sfrutti i pacchetti e la configurazione appropriati (cartella ssh_cluster/). Monitorare l’output per identificare eventuali bug o crash specifici del cluster (es. path dataset, differenze di versione).
Analizzare i log e le curve di reward di TD3: confermare che l’algoritmo funziona senza errori ma notare come la performance si stabilizza e non cresce più di tanto (plateau di reward). Identificare quando e dove avviene il plateau per orientare gli interventi di tuning.
Effettuare tuning degli iperparametri principali di TD3 per migliorare la convergenza:
Provare a variare la learning rate (ad esempio abbassandola se si osservano oscillazioni instabili o aumentandola leggermente se l’apprendimento è troppo lento) – un learning rate troppo alto può causare divergenze/oscillazioni, mentre uno troppo basso rallenta l’apprendimento e può portare l’agente a sfruttare precocemente strategie sub-ottimali
ai.stackexchange.com
.
Regolare la dimensione del replay buffer (se TD3 la usa) e la frequenza di aggiornamento del target network (τ) per vedere se un aggiornamento più morbido migliora la stabilità.
Modificare il batch size: un batch più grande può ridurre la varianza degli aggiornamenti ma potrebbe rendere l’apprendimento più lento, mentre batch piccoli aumentano la rumorosità ma reagiscono più rapidamente alle nuove esperienze.
Esplorare diversi valori di gamma (fattore di sconto) se il focus degli reward deve essere più sul breve o lungo termine.
Annotare sistematicamente i risultati di ogni variazione per identificare configurazioni migliori. Potrebbe emergere che TD3 converge solo entro un range ristretto di iperparametri, il che richiede molte prove e pazienza
jonathan-hui.medium.com
.
Valutare l’effetto del rumore esplorativo di TD3 (OU-noise): se l’agente resta bloccato in politiche sub-ottimali, aumentare la scala del rumore o la varianza iniziale per favorire maggiore esplorazione di strategie alternative.
Multi-run & seed: Rieseguire TD3 più volte con seed diversi per accertarsi che i risultati non siano dovuti a un singolo seed sfortunato. Notare che in RL run diversi con soli seed differenti possono portare a esiti molto variabili (diversi massimi locali)
jonathan-hui.medium.com
.
SAC e PPO (esecuzione in locale):
Integrare e finalizzare l’implementazione di SAC e PPO assicurandosi che il codice funzioni in locale. Risolvere eventuali errori riscontrati (ad es. errori nel leggere lo schema JSON dell’ambiente CityLearn: se il file schema della challenge AIcrowd non è accessibile, usare o adattare lo schema fornito nella versione CityLearn 2.1.2 come già iniziato).
Dopo il debug, eseguire prove di training con SAC e PPO sull’ambiente CityLearn con la reward personalizzata. Verificare subito se l’agente mostra segni di apprendimento (trend positivo nella reward) o se rimane piatto. In caso di mancato apprendimento (“l’agente non apprende nulla”), procedere con le seguenti azioni:
Semplificare il problema inizialmente (vedi sezione reward custom) per ridurre la complessità affrontata da SAC/PPO, così da capire se l’algoritmo funziona in condizioni più semplici.
Effettuare tuning degli iperparametri anche per SAC e PPO:
Learning rate: analogamente a TD3, provare valori diversi (es. $3\cdot10^{-4}$ predefinito, poi $1\cdot10^{-4}$ o $1\cdot10^{-3}$) per vedere impatti su stabilità e gradiente di apprendimento
ai.stackexchange.com
.
Batch size e update frequency: per PPO in particolare, regolare il numero di passi per aggiornamento e il minibatch size usato nell’ottimizzazione. Un batch maggiore può stabilizzare PPO, ma richiede più esperienza prima di ogni aggiornamento.
Entropia: aumentare il coefficiente di entropia (o parametro alpha in SAC, se non è auto-tuned) per incentivare l’esplorazione se l’agente si mostra troppo conservativo. Una maggiore esplorazione aiuta a uscire da possibili massimi locali incoraggiando policy più diversificate
jonathan-hui.medium.com
.
Discount factor (gamma): se pertinenti, provare a ridurre leggermente gamma (es. da 0.99 a 0.95) per dare più peso ai reward immediati, facilitando l’apprendimento di comportamenti di breve periodo (può aiutare quando la sequenza di ricompense è molto diluita nel tempo).
Clip range e GAE (per PPO): se PPO fatica, controllare i parametri specifici come il clipping ratio degli update (es. 0.2) e i parametri dell’Advantage generalizzato (lambda) per garantire che l’aggiornamento della policy non sia né troppo rigido né troppo permissivo.
Eseguire grid search o random search su combinazioni di iperparametri chiave se il tempo lo consente, registrando i risultati. In letteratura è noto che gli algoritmi RL richiedono molte prove per trovare iperparametri ottimali (spesso più che nel deep learning supervisato)
jonathan-hui.medium.com
.
Monitorare attentamente i grafici di reward/metriche episodio per ogni run di SAC/PPO e confrontarli: identificare eventuali segnali di miglioramento (anche se piccoli) rispetto al caso totalmente piatto.
Confronto algoritmi: Una volta ottenute run stabili, confrontare TD3, SAC e PPO in termini di performance (es. reward cumulativa media, o valori dei KPI finali). Notare differenze: ad esempio, TD3 potrebbe stabilizzarsi su un valore di reward diverso da SAC/PPO. Documentare queste differenze per discuterle (es. TD3 vs PPO: uno potrebbe essere più stabile, l’altro più incline a esplorare, ecc.).
Integrazione nel repository: Dopo il debug e tuning iniziale in locale, consolidare il codice aggiornato di SAC e PPO nel repository (committare/pushare le implementazioni funzionanti). Aggiungere istruzioni nei README su come eseguire questi algoritmi localmente, analogamente a quanto fatto per TD3 su cluster.
2. Ottimizzazione della reward custom (AIcrowdControl) con approccio incrementale sui KPI
Analisi della reward multi-obiettivo: Riesaminare la funzione di reward custom implementata (AiCrowdControl.py) che combina otto KPI (Key Performance Indicators) – ad esempio: emissioni di carbonio, comfort termico, ramping, load factor, picchi di domanda, resilienza termica, energia non servita, ecc. – con pesi specifici per fase
aicrowd.com
aicrowd.com
. Assicurarsi di aver compreso il significato di ciascun KPI e la formula con cui viene calcolato nel contesto di CityLearn (rifacendosi alla descrizione della challenge).
Validazione implementazione: Verificare che ogni componente della reward sia implementato correttamente secondo le specifiche della challenge (ad es. controllo che le formule in ControlTrackReward corrispondano a quelle attese
aicrowd.com
aicrowd.com
). Eventualmente aggiungere assert o test unitari per calcolare manualmente un KPI in una situazione semplice e confrontare col valore prodotto dalla funzione.
Approccio incrementale sui KPI: Non iniziare subito includendo tutti i componenti della reward insieme, ma testare l’apprendimento in modo incrementale:
Fase 1: Eseguire l’addestramento dell’agente utilizzando solo 1-2 KPI alla volta. Ad esempio, iniziare con un sotto-obiettivo isolato (come minimizzare il discomfort U oppure le emissioni G) con pesi degli altri KPI impostati a zero. Osservare se l’agente riesce effettivamente a migliorare quel singolo obiettivo nel tempo (ci si aspetta un segnale di reward più semplice e mirato).
Fase 2: Una volta verificato che l’agente impara su obiettivi individuali, aggiungere gradualmente un altro KPI nella funzione di reward. Esempio: prima ottimizzare comfort (U) da solo, poi comfort + emissioni (U+G). Monitorare l’andamento: l’aggiunta del secondo obiettivo causa nuovamente stagnazione? In caso affermativo, può indicare conflitto tra obiettivi o scale diverse.
Fase 3: Continuare ad aggiungere componenti incrementando la complessità (ad es. introdurre i KPI di grid stability come ramping R o load factor L solo dopo aver gestito comfort/emissioni). Identificare quale KPI o combinazione causa il blocco dell’apprendimento (“con quale obiettivo specifico si blocca” come suggerito). Questo aiuta a focalizzare eventuali interventi (es. se aggiungendo il KPI di picco di consumo l’agente collassa, forse quel segnale è dominante o difficoltoso).
Analisi conflitti e scale: Per ogni step incrementale, analizzare se alcuni termini di reward sovrastano gli altri. Potrebbe essere utile normalizzare o riscalare alcuni KPI (ad es. portandoli su range simili) o ribilanciare i pesi PhaseWeights per assicurarsi che nessun singolo termine domini la ricompensa totale.
Logging dettagliato: Durante questi esperimenti, loggare separatamente il contributo di ciascun KPI al reward totale ad ogni episodio. In questo modo, se l’agente non converge quando ci sono più obiettivi, si può vedere quale componente resta costantemente negativo o non migliorato.
Semplificazione dell’ambiente (opzionale, se possibile): Se anche con 1-2 KPI l’agente fatica, considerare di semplificare l’ambiente CityLearn per test. Ad esempio, usare un sottoinsieme di edifici, o un orizzonte temporale più breve, per vedere se l’agente riesce a imparare in un ambiente meno complesso. Questo aiuterebbe a capire se il problema è la complessità ambientale (17 edifici, molte variabili) o la reward in sé.
Iterare sul design della reward: In base ai risultati incrementali, valutare modifiche alla reward:
Se certi termini creano instabilità, considerare di applicare reward shaping, ad esempio aggiungendo un termine ausiliario più denso. (Esempio: se la resilienza S è difficile da apprendere perché gli outage sono rari, si potrebbe aggiungere un piccolo incentivo intermedio legato allo stato della batteria per guidare l’agente).
Se l’agente ignora un KPI, incrementare il peso relativo di quel KPI nei PhaseWeights per un periodo, per poi eventualmente riportarlo al valore originale una volta che l’agente inizia a considerarlo.
Documentare le prove fatte (quali KPI attivati in quale ordine, quali pesi) e i relativi risultati, così da costruire una storia di come la reward custom influisce sull’apprendimento. Questo sarà utile sia per capire il problema sia per spiegarlo nella documentazione e presentazione.
Convergenza finale: L’obiettivo è ottimizzare progressivamente tutti i KPI. Se il tempo lo consente, dopo l’approccio incrementale provare di nuovo con tutti i componenti attivi ma tenendo conto dei tuning fatti: ad es. usare i pesi PhaseWeights finali e gli iperparametri che hanno dato i migliori risultati parziali. Potrebbe darsi che l’agente, “allenato” gradualmente, ora riesca a ottenere un risultato migliore combinato. In caso contrario, si avrà comunque raccolto abbastanza insight per spiegare perché l’algoritmo fatica a convergere con quella reward complessa.
3. Organizzazione del codice e documentazione
Pulizia dei notebook: Rivedere tutti i Jupyter Notebook del progetto ed eseguire una pulizia approfondita:
Rimuovere codice morto, celle duplicate o risultati di vecchie run non più rilevanti, in modo che ogni notebook presenti un flusso chiaro.
Aggiungere commenti esplicativi nelle celle di codice cruciali, spiegando l’obiettivo di quella sezione (es. setup ambiente, training agente, visualizzazione risultati).
Se alcuni notebook erano solo prove intermedie o debug, valutare se integrarli in un notebook principale o spostarli in una cartella archive/ per non distrarre.
Output grafici: assicurarsi che grafici importanti (learning curve, ecc.) vengano visualizzati correttamente nei notebook. Eventualmente aggiornare titoli, etichette assi e legende per chiarezza, dato che questi notebook potrebbero servire anche per ricavare materiale per la presentazione.
Verificare che il codice nei notebook sia riutilizzabile: ad esempio, parametri chiave definiti all’inizio (seed, episodi, ecc.) in modo da poter rifare esperimenti modificando facilmente i valori.
Revisione dei README e guide: Aggiornare la documentazione testuale in README.md e in eventuali guide specifiche nei folder:
Folder ssh_cluster/: controllare il README/guida per l’esecuzione su cluster. Includere i passaggi per setup (es. creazione env con conda o pip, variabili d’ambiente se necessarie), come lanciare lo script TD3 in cluster, e indicare i risultati attesi/salvati. Se mancava qualche dettaglio (ad esempio specificare che l’algoritmo TD3 funziona ma raggiunge un plateau), aggiungerlo brevemente.
Folder AICrowd/: completare o creare un README per questa parte. Spiegare la logica della custom reward AIcrowd Control: descrivere in parole semplici cosa fa AiCrowdControl.py (es. “calcola la ricompensa combinando metriche di comfort, emissioni, rete e resilienza secondo i pesi della sfida NeurIPS CityLearn”). Includere istruzioni su come utilizzare questa reward (es. come importarla o attivarla nell’ambiente CityLearn). Inoltre, documentare come eseguire gli algoritmi SAC e PPO localmente: requisiti, comandi di avvio (magari usando Stable Baselines3 se è stato usato, o script Python dedicati), e dove vengono salvati i risultati.
Formato e coerenza: assicurarsi che tutti i file README abbiano uno stile coerente (sezioni ben strutturate: Introduzione, Setup, Istruzioni di Esecuzione, Risultati/Output, ecc., eventualmente in italiano se il progetto è in italiano, oppure in inglese se richiesto). Correggere typo e aggiornare riferimenti a file o cartelle se la struttura è cambiata.
Documentare il problema di convergenza: Redigere una sezione di documentazione (ad esempio in un file README_Troubleshooting.md o all’interno del report principale) in cui si spiega perché l’algoritmo potrebbe non convergere bene sul problema dato. Questo punto è cruciale da preparare in risposta sia alle possibili domande sia per onestà di report. In questa sezione:
Descrivere i sintomi riscontrati: ad es. “nonostante molteplici tentativi di tuning, gli algoritmi di RL non mostrano miglioramenti significativi nella ricompensa cumulativa, indicando difficoltà di apprendimento”. Citare sia TD3 (plateau di reward) che SAC/PPO (assenza di trend positivo) con riferimenti a grafici o valori quantitativi.
Elencare e spiegare i possibili motivi di questa mancata convergenza:
Complessità della task: CityLearn con reward multi-obiettivo è un ambiente difficile, l’agente potrebbe trovarsi a dover bilanciare obiettivi conflittuali (es. comfort vs. efficienza energetica) e questo rende la superficie di reward molto irregolare con più ottimi locali. È noto che gli algoritmi di policy gradient possono rimanere intrappolati in massimi locali, specialmente in problemi complessi
jonathan-hui.medium.com
.
Iperparametri sub-ottimali: spiegare che in RL gli iperparametri hanno un impatto enorme. È possibile che la giusta combinazione di learning rate, entropia, ecc. sia molto specifica e non sia stata trovata nel tempo disponibile. (Si può menzionare ad esempio: learning rate inadeguato – troppo alto crea instabilità, troppo basso rallenta e può impedire di uscire da politiche iniziali
ai.stackexchange.com
).
Esplorazione insufficiente: notare che l’agente potrebbe non esplorare abbastanza strategie nuove. Ad esempio, PPO senza un adeguato coefficiente di entropia o SAC con alpha troppo basso potrebbero convergere prematuramente. Aggiungere che un’esplorazione più ampia (per es. aumentando l’entropia o aggiungendo rumore alle azioni) potrebbe aiutare a sfuggire ai local optima
jonathan-hui.medium.com
.
Limiti di tempo/addestramento: sottolineare se l’addestramento è stato fatto solo per un numero relativamente limitato di episodi/epoch rispetto a quanto richiesto per vedere progressi in problemi complessi. Forse i progressi sarebbero visibili con molte più iterazioni (ma vincoli di tempo computazionale l’hanno impedito).
Altri fattori: menzionare qualsiasi altra difficoltà incontrata, ad esempio bug nascosti (non trovati) che potrebbero aver impedito all’algoritmo di vedere correttamente certe osservazioni o un preprocessamento dei dati non ottimale, ecc., anche se non si hanno certezze.
Per ogni punto sopra, indicare come si è provato a porvi rimedio (tuning manuale, approccio incrementale sulla reward, semplificazione progressiva) e i risultati di tali tentativi. Anche il fatto che “è normale che un algoritmo non converga bene in alcuni scenari” può essere menzionato, citando che si è investita circa una settimana in tentativi di tuning senza miglioramenti sostanziali.
Concludere la sezione documentando le lesson learned: ad esempio, “l’importanza di una buona definizione della funzione di reward in domini multi-obiettivo” o “quanto sia delicato il tuning in algoritmi di deep RL” ecc. Questo tornerà utile anche nelle conclusioni della presentazione.
Migliorie al codice: Oltre alla documentazione, fare un ultimo passaggio di refactoring leggero se necessario: rinominare variabili poco chiare, strutturare il repository in modo pulito (ad es. cartelle env/, agents/, utils/ se applicabile). Aggiungere eventualmente un file CHANGELOG.md con elenco delle modifiche principali effettuate durante lo sviluppo del progetto, per tracciare l’evoluzione (utile se qualcun altro guarderà il repository).
Verifica finale: Dopo pulizia e documentazione, testare da zero le istruzioni dei README (eseguire realmente i comandi suggeriti per assicurarsi che funzionino in un ambiente pulito). Così ci si assicura che il progetto sia riproducibile e ben organizzato in vista della consegna.
4. Creazione di una cartella results/ con grafici e metriche delle varie run
Strutturare la cartella: Creare una directory denominata results/ nella root del progetto. All’interno, organizzare eventualmente sottocartelle per tipologia di esperimento o algoritmo (es. results/TD3_cluster/, results/SAC_local/, results/PPO_local/, results/reward_incremental/ etc.), in modo da mantenere i risultati ben distinti e facili da consultare.
Salvataggio dei grafici chiave: Per ogni tipo di run effettuata, salvare i grafici più rappresentativi:
Learning curve: grafico della reward cumulativa media per episodio nel tempo (per TD3, SAC, PPO). Questo evidenzierà l’andamento dell’apprendimento (es. eventuale plateau). Salvare in formato .png (es. learning_curve_TD3.png).
Andamento dei KPI: se possibile, grafici che mostrano l’andamento di alcuni KPI importanti durante l’episodio o confronti prima/dopo. Ad esempio, un grafico della curva di carico elettrico di un edificio con e senza controllo RL, oppure un confronto delle emissioni totali nell’episodio baseline vs controllato dall’agente. Questi possono essere utili per mostrare effetti concreti delle policy apprese (anche se sub-ottimali).
Confronto algoritmi: un grafico combinato che sovrappone le performance (reward) di TD3 vs SAC vs PPO, se significativo, per illustrare differenze di comportamento.
Incremental reward tests: se si sono eseguiti test con reward parziali, includere grafici che mostrano, ad esempio, come la performance sul KPI comfort migliora quando si ottimizza solo quello, rispetto a quando si aggiungono altri obiettivi (questo potrebbe essere mostrato con bar chart o curve separate).
Metriche numeriche: Oltre ai grafici, preparare file di testo/CSV con alcune metriche riassuntive: ad esempio, score finale o media degli ultimi N episodi per ogni algoritmo e configurazione. Salvare in results/ dei piccoli report (es. TD3_results.txt contenente: reward finale, valori medi dei vari KPI a fine training, ecc.). Questi numeri possono servire per referenza rapida durante la presentazione o per rispondere a domande puntuali (“quanto ha ridotto le emissioni l’agente?” etc.).
Inserimento dati significativi: Popolare la cartella results/ anche se i risultati non sono ottimali, per scopi illustrativi. Ad esempio, includere un grafico di reward che mostra il plateau di TD3 sul cluster, accompagnato magari da una linea orizzontale che indichi la performance di un baseline (se disponibile, come “no storage control”). Questo può aiutare a illustrare che l’algoritmo purtroppo non supera di molto il comportamento base.
Screenshot e figure della simulazione: Se rilevante, aggiungere immagini illustrative, ad esempio diagrammi dell’ambiente (come schema del flusso energetico in CityLearn) o screen della simulazione (tipo andamento temperatura vs setpoint in un edificio). Questo arricchisce la presentazione. (Queste immagini vanno nella presentazione, ma è utile salvarle in results/ per tenerle a portata di mano).
Automatizzare salvataggi: Modificare il codice di training affinché, al termine di ogni run importante, salvi automaticamente i grafici o log necessari nella cartella results/. Ad esempio, usare matplotlib plt.savefig() dopo il plot, salvare i modelli addestrati (se utile) e i log di console (redirect output in file) per futura analisi. In questo modo si evitano dimenticanze e si ha un arsenale di materiale pronto per la presentazione.
Verifica contenuto: Controllare che tutti i file inseriti in results/ siano comprensibili a posteriori: nominare i file in modo descrittivo e, volendo, aggiungere un breve README.md dentro results/ che elenca i file e il loro significato. Questo tornerà utile quando si selezioneranno gli elementi da inserire nelle slide.
Backup: Effettuare un backup della cartella results/ (ad es. caricandola su Drive o repository) per sicurezza, dato che contiene ore di calcolo e grafici preziosi.
5. Strutturazione della presentazione finale (circa 15 minuti)
(Preparare una presentazione chiara e concisa, seguendo lo schema classico suggerito dal relatore. Suddividere in sezioni con slide dedicate a ciascun punto chiave.)
Introduzione – Problema e motivazione (CityLearn):
Spiegare il contesto e il problema affrontato: “Come gestire in modo intelligente l’energia in un insieme di edifici (microgrid urbana) mantenendo il comfort e riducendo emissioni/costi”. Introdurre brevemente l’ambiente CityLearn (NeurIPS CityLearn Challenge) e perché è rilevante (es. importante per efficienza energetica, integrazione rinnovabili, smart grid).
Motivazione: perché usare il Reinforcement Learning per questo problema? Sottolineare che le strategie tradizionali (regole fisse, ottimizzazione classica) faticano ad adattarsi a dinamiche complesse, mentre un agente RL può apprendere una politica di controllo ottimizzando obiettivi multipli in modo adattivo.
Descrivere in breve la sfida specifica (Control Track): menzionare che c’era una custom reward multi-obiettivo (comfort, emissioni, ecc.) da ottimizzare – questo prepara il terreno per spiegare la reward custom implementata.
(Slide tipica: elenco bullet di obiettivi, immagine schematica di edifici/batterie, ed elenco dei KPI da ottimizzare per dare concretezza al pubblico).
Metodo – Approccio RL model-free:
Fornire una panoramica del metodo scelto: algoritmi model-free RL per controllo continuo. Elencare brevemente gli algoritmi provati: TD3 (Twin Delayed DDPG), SAC (Soft Actor-Critic), PPO (Proximal Policy Optimization). Una frase su ciascuno: es. TD3 e SAC sono algoritmi off-policy a gradiente di politica per azioni continue, PPO è on-policy con trust region.
Giustificare la scelta di provare più algoritmi: il dominio è complesso, non era scontato quale avrebbe appreso meglio; inoltre, TD3/SAC vs PPO permettono di confrontare off-policy vs on-policy nel contesto.
Descrivere l’agente e ambiente: un singolo agente che controlla tutti gli edifici (se così è stato impostato) oppure agente multi-edificio? Specificare: “Abbiamo adottato un single-agent RL che controlla in modo centralizzato le risorse (batterie, pompe di calore) di tutti gli edifici. L’osservazione include lo stato di ogni edificio (…), l’azione include setpoint/dispatch energetici. L’obiettivo è massimizzare la reward definita dal challenge.”
Spiegare la custom reward AIcrowd: dedicare un breve focus alla funzione di reward implementata – elencare i componenti KPI (magari mostrando la formula a livello alto o uno schema a blocchi con i pesi). Far capire che è una reward molto articolata, somma pesata di più obiettivi (comfort, efficienza, resilienza…). Sottolineare che questa complessità della reward è stata una delle principali sfide: dover equilibrare obiettivi multipli.
Menzionare dettagli di implementazione: es. “Abbiamo utilizzato la libreria CityLearn 2.1.2, con un ambiente di 17 edifici, horizon di 1 anno simulato a timestep orario” (se pertinente). Indicare che il training è stato svolto sia in locale che su cluster per accelerare (TD3 ha richiesto esecuzione su cluster per tempi computazionali, etc.).
(Slide tipiche: schema dell’algoritmo RL che interagisce con l’ambiente CityLearn, e magari tabella con i principali iperparametri usati per ciascun algoritmo – learning rate, gamma, ecc. – per trasparenza).
Esperimenti – Configurazione e Tuning:
Descrivere come sono stati organizzati gli esperimenti. Ad esempio: “Abbiamo addestrato ciascun algoritmo per X episodi (corrispondenti a Y anni simulati) usando la reward multi-obiettivo. Per TD3 il training è stato distribuito sul cluster (data la maggiore domanda computazionale), mentre SAC e PPO sono stati testati in locale.”
Spiegare il processo di tuning iterativo: menzionare che inizialmente le performance erano scarse, e quindi si è proceduto a regolare iperparametri e testare modifiche. Dare qualche esempio concreto: “Ad esempio, inizialmente PPO non mostrava alcun apprendimento; abbiamo quindi aumentato il coefficiente di entropia e diminuito la learning rate, il che ha leggermente migliorato la stabilità (anche se non sufficiente a convergere).” Oppure “TD3 si stabilizzava su un reward quasi costante; abbiamo provato a aumentare il noise di esplorazione senza però ottenere miglioramenti sostanziali.”
Parlare dell’approccio incrementale sulla reward negli esperimenti: spiegare che viste le difficoltà, avete provato a far apprendere l’agente prima su obiettivi semplificati. Descrivere questo tentativo e cosa avete osservato (es. l’agente riesce a ottimizzare comfort da solo, ma aggiungendo emissioni crolla la performance, indicando conflitto). Questa parte dimostra metodo scientifico nel diagnosticare il problema.
(Slide: tabella o lista delle configurazioni provate – ad es. “TD3 base vs TD3 lr ridotta vs TD3 noise aumento”, “PPO base vs PPO entropia↑”, ecc., con indicazione qualitativa del risultato; oppure schema a blocchi del processo di tuning).
Evidenziare che per questioni di tempo computazionale, non è stato possibile fare una ricerca esaustiva su tutti gli iperparametri, ma ci si è concentrati su quelli più influenti (learning rate, esplorazione, ecc.). Citare che letteratura suggerisce learning rate e fattore di sconto tra i più critici da sintonizzare
ai.stackexchange.com
.
Risultati:
Presentare i risultati ottenuti dagli algoritmi. Sii onesto e chiaro: se i risultati non sono ottimali, è meglio mostrarlo apertamente insieme a spiegazioni, piuttosto che sorvolare.
Mostrare il grafico della learning curve di TD3: evidenziare il plateau. Ad esempio: “Come si vede, TD3 raggiunge rapidamente una ricompensa media attorno a X e non migliora ulteriormente nonostante il proseguire dell’addestramento.” Questo è un punto per spiegare brevemente che l’algoritmo si è probabilmente fermato in un ottimo locale o ha saturato le proprie capacità su quella definizione di reward.
Mostrare le curve (o metriche) di SAC e PPO: se entrambe non imparano (linea piatta), farlo vedere. “Qui vediamo la ricompensa episodica di SAC e PPO rimanere essenzialmente piatta attorno a 0 (o al valore base) per tutto il training.” Ciò conferma la difficoltà. Se uno dei due avesse avuto un trend leggermente migliore dell’altro, sottolinearlo (es. “SAC sembra leggermente più stabile di PPO ma nessuno dei due mostra un chiaro trend ascendente”).
Se disponibili, presentare risultati numerici: ad esempio “Alla fine dell’addestramento, il punteggio totale (ScoreControl) raggiunto dall’agente è circa 0.65, confrontato con 0.62 di un agente random”, oppure “il comfort medio migliorato del 5% ma al costo di un aumento del 10% delle emissioni”. Qualche numero dà concretezza, anche se non brillante, e fa capire il trade-off.
Introdurre possibili motivi dei risultati: collegare ai punti di documentazione. “Abbiamo ragione di credere che l’agente non converga a causa della complessità della ricompensa. Nonostante aggiustamenti, l’agente tende a massimizzare solo alcuni aspetti (es. comfort) a scapito di altri, restando bloccato prima di raggiungere una soluzione ottimale globale.”
Se avete effettuato l’esperimento incrementale sulla reward, menzionare i risultati parziali: ad es. “In figura mostriamo l’apprendimento sul solo KPI di comfort: in questo caso l’agente è riuscito a ridurre drasticamente le ore di discomfort. Ciò indica che il sotto-problema è risolvibile; tuttavia, aggiungendo gli altri obiettivi, il task diventa molto più arduo e la performance non cresce.” Questo dettaglio dimostra che il metodo funziona in principi ma la multi-obj lo complica.
Menzionare se c’è stato qualche miglioramento tra algoritmi: “Tra le tecniche provate, TD3 (pur non convergendo all’optimum) ha fornito le migliori performance assolute, suggerendo che l’off-policy replay abbia aiutato a estrarre più informazione. PPO invece potrebbe richiedere più tuning per questo scenario.”
(Slide: grafici dal folder results/ – ben etichettati – e magari una tabella riassuntiva con “TD3: reward finale = X, SAC = Y, PPO = Z” e magari “baseline = W” se esiste un riferimento, così da avere un confronto immediato).
Conclusioni e proposte di miglioramento:
Riassumere brevemente ciò che si è ottenuto e appreso: “In sintesi, l’applicazione di algoritmi di RL model-free al problema CityLearn ha mostrato le difficoltà intrinseche di bilanciare molteplici obiettivi. L’agente ha appreso solo parzialmente il comportamento desiderato (es. ha mantenuto il comfort ma senza ridurre sostanzialmente le emissioni).”
Sottolineare comunque gli aspetti positivi o i progressi: ad esempio, “Questo lavoro ha permesso di implementare correttamente l’infrastruttura di training in un ambiente complesso e di capire l’impatto dei vari termini di reward. Si è visto che alcuni algoritmi (TD3) mantengono stabilità, mentre altri necessitano di ulteriore messa a punto.”
Proposte di miglioramento futuro: presentare alcune idee per lavori successivi che potrebbero superare i problemi riscontrati:
Reward shaping avanzato: studiare metodi per combinare gli obiettivi in modo più graduale (ad es. tecniche di curriculum learning dove prima si addestra su un obiettivo poi si aggiungono gli altri, oppure metodi di scalarization dinamica dei multi-obiettivo che variano i pesi durante il training).
Algoritmi alternativi: menzionare la possibilità di usare algoritmi model-based o con esplorazione migliorata. Ad esempio, provare approcci di Hierarchical RL (un livello decide priorità tra comfort/emissioni, un altro controlla le azioni base) o usare Multi-Agent RL (un agente per edificio o per obiettivo) per vedere se la decomposizione del problema aiuta. Oppure considerare metodi come MPC (controllo predittivo) come baseline avanzata, se la sfida lo permetteva, per confrontare i risultati.
Più risorse computazionali: suggerire che una convergenza migliore potrebbe richiedere più training (es. addestrare per 10^6 timestep invece di 10^5) e tuning automatizzato (magari tramite Bayesian optimization degli iperparametri).
Feature engineering: valutare se fornire all’agente osservazioni più informative (es. pre-elaborare alcune features, o aggiungere indicazioni sul prossimo outage nella simulazione di resilienza) potrebbe aiutare l’apprendimento.
Concludere con una nota positiva: “Questo progetto, pur con le sue sfide, ha contribuito a comprendere a fondo il problema e getta le basi per futuri sviluppi che potranno migliorare la gestione energetica intelligente.”
(Slide finale: elenco bullet delle principali migliorie future, ringraziamenti.)
Timing: Provare l’esposizione cronometrata in anticipo, per assicurarsi di rientrare nei ~15 minuti. Orientativamente: 2 min introduzione, 3 min metodo, 4-5 min esperimenti e risultati, 2 min conclusioni. Preparare eventualmente slide extra di backup con dettagli tecnici (formule reward, tabelle iperparametri completi) da utilizzare solo se espressamente chiesti nelle domande.
6. Preparazione alle domande comuni del relatore (reward, tuning, logica dei test)
Domande sulla reward custom: Prepararsi a spiegare nel dettaglio la reward AIcrowd e le sue implicazioni:
Come è formulata esattamente la funzione di reward? – Sii pronto a descrivere a parole (o anche mostrando una formula se necessario) come vengono calcolati i punteggi di comfort, emissioni, rete, resilienza e come vengono pesati. Ad esempio: “La reward istantanea è data da $0.3 * Score_{Grid} + 0.1 * Score_{Emissions} + 0.6 * Score_{Comfort}$ in fase I (resilienza esclusa), dove Score_{Grid} a sua volta è media di 4 KPI di rete...”.
Perché questa formulazione? – Spiegare che deriva dalla competizione CityLearn, dunque è stata fornita per incentivare un bilanciamento tra obiettivi. Sottolineare che non è progettata da noi ma scelta dagli organizzatori, quindi eventuali difficoltà fanno parte della sfida stessa.
Quali difficoltà ha causato? – Evidenziare che avere tanti termini ha reso il paesaggio delle ricompense complesso e ha portato a conflitti (es. migliorare comfort peggiora emissioni, ecc.), rendendo l’apprendimento instabile. Puoi citare il fatto che l’agente spesso si è bloccato in massimi locali soddisfacendo solo parzialmente gli obiettivi. Questo mostra la comprensione profonda del perché la reward è un problema.
Avete considerato semplificare o modificare la reward? – Qui parlerai dell’approccio incrementale: spiega che sì, avete testato l’agente su ricompense semplificate (come comfort solo) per vedere se imparava, e infatti così funzionava meglio. Ciò dimostra che il problema non è l’algoritmo in sé ma la combinazione simultanea di troppi obiettivi. Potrebbero chiedere: “Perché non usare un metodo multi-obiettivo esplicito?” – Rispondi magari che sarebbe interessante (ad es. Pareto RL o assegnare policy diverse per obiettivi) ma era fuori portata in questo progetto iniziale, e avete seguito la definizione della challenge.
Domande sul tuning e risultati:
Quali parametri avete dovuto tunare maggiormente e perché? – Elenca i principali: learning rate, batch size, coefficiente di esplorazione/entropia, ecc. e per ognuno spiega il razionale: es. “Abbiamo ridotto la learning rate perché vedevamo oscillazioni nel reward – infatti un learning rate troppo alto in un ambiente con tante penalizzazioni può causare comportamenti instabili
ai.stackexchange.com
. Abbiamo aumentato l’entropia per spingere l’esplorazione dato che l’agente sembrava convergere prematuramente a una politica sub-ottimale.” Menziona anche parametri come gamma o il numero di neuroni della rete se li avete toccati.
Come avete scelto gli iperparametri finali? – Spiega che per limite di tempo non avete potuto esplorare exhaustively tutto, quindi avete seguito best practice della letteratura (ad esempio valori default di Stable Baselines come punto di partenza) e poi li avete regolati manualmente uno alla volta osservando gli effetti. Se avete fatto tentativi sistematici (grid search su 2-3 valori), citatelo.
Perché pensate che l’algoritmo non converga? – Questa è probabile come domanda diretta. La risposta dovrebbe riassumere quanto documentato: “Riteniamo sia dovuto alla complessità della ricompensa multi-obiettivo che rende difficile la ricerca di una politica ottima. In pratica l’agente ottiene segnali contrastanti e resta in un compromesso (massimo locale). Anche la necessità di esplorare molto di più potrebbe essere un fattore: con parametri standard l’agente non esplora abbastanza strategie alternative e si accontenta di un risultato non ottimo. Abbiamo visto che neanche variando iperparametri entro un certo limite si sbloccava la situazione, il che suggerisce che servirebbero forse approcci più radicali (come ripensare la reward o fornire un segnale di apprendimento più guidato).” In altre parole, dare una spiegazione completa combinando i fattori: multi-obiettivo, exploration vs exploitation, iperparametri, ecc., mostrando che hai un quadro chiaro del perché.
Cosa avreste potuto fare di diverso per far apprendere meglio l’agente? – Domanda collegata: rispondi con le proposte di miglioramento già discusse. Ad esempio: “Un’idea sarebbe usare un approccio in due fasi: prima addestrare una policy per comfort, un’altra per efficienza, poi combinarle. Oppure provare un algoritmo differente come XXX. Inoltre, disporre di più tempo per far girare l’addestramento (magari settimane invece che giorni) e un tuning automatizzato probabilmente avrebbe giovato.” Far capire che hai pensato a possibili soluzioni oltre a quello che hai implementato.
La scelta degli algoritmi: perché TD3, SAC, PPO? – Giustifica che coprono una buona varietà di approcci noti per i continui. TD3/SAC sono state scelte perché hanno avuto successo in problemi continui (riferimento: Mujoco, ecc.) e SAC in particolare è off-policy e con entropy tuning – poteva teoricamente gestire bene trade-off esplorazione. PPO è stato provato perché è robusto on-policy e spesso funziona in molti scenari; inoltre era interessante confrontare on-policy vs off-policy. Se chiedono perché non DQN (discreto) o perché non metodi evolutivi, spiega che l’azione è continua (DQN non va bene) e che avete voluto restare nel campo del deep RL data-driven piuttosto che optare per metodi non apprendimento (anche se un MPC poteva essere un confronto, ma la focus era sul RL).
Logica dei test e validazione: Potrebbero chiedere come avete verificato il comportamento dell’agente. Ad esempio: “Avete testato l’agente in un ambiente diverso o con dati diversi da quelli di training?” – Probabilmente, essendo una simulazione, non c’è distinzione train/test come in sup. learning, ma puoi dire che ogni episodio è indipendente e valutato su metriche aggregate. Nella challenge originale, il modello sarebbe testato su scenari nascosti Phase II, ma nel progetto ci siamo concentrati su Phase I. Se avete fatto più run con diversi random seed, menzionalo come forma di test di robustezza.
Domande sui dettagli implementativi: Es. “Quanti neuroni ha la rete dell’agente?”, “Che architettura avete usato?” – avere pronti questi dettagli: dimensioni dei layer (ad es. 2 hidden layers da 256 neuroni), funzioni di attivazione (ReLU), ecc. Non è detto lo chiedano, ma essendo un lavoro di tesi può capitare. Anche: “Che macchina avete usato per l’addestramento?” – dire che TD3 ha girato su cluster (specifica CPU/GPU se rilevante), SAC/PPO local su un PC con tot CPU/GPU, ecc.
Domande di curiosità/future work: Es. “Secondo te, aggiungere X avrebbe aiutato?” – Rispondere mostrando apertura mentale: “Può darsi. Ad esempio X potrebbe fornire all’agente un’informazione che attualmente non ha. Sarebbe interessante provare.” Mantenere un atteggiamento riflessivo e orientato al miglioramento.
Strategia generale: Per ciascuna domanda, prendere un secondo per strutturare la risposta in modo chiaro e conciso. Se possibile, collegare la risposta a qualcosa già mostrato nelle slide (es. “Come vedevamo nel grafico tale, ...”), così da rendere tutto coerente. Portare eventualmente copie cartacee della documentazione tecnica (o avere a portata di mano i notebook puliti) per riferimento, nel caso venga posta una domanda molto specifica: poter consultare rapidamente i risultati numerici o il codice all’occorrenza è rassicurante.
Simulazione di Q&A: Fare una prova con un collega/amico simulando le domande del relatore. Allenarsi a rispondere senza esitazioni e verificare di saper spiegare anche i concetti più spinosi (reward multi-obiettivo, iperparametri) in modo comprensibile. Avere pronte analogie semplici se servono (es. “la learning rate in RL è come la velocità con cui l’agente impara: troppo veloce e sbanda, troppo lenta e non si muove”). Questo aiuta a sentirsi più sicuri il giorno della discussione.
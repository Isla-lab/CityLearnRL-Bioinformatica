{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13i7KQ9t-CV8"
   },
   "source": [
    "# CityLearn: A Tutorial on Reinforcement Learning Control for Grid-Interactive Efficient Buildings and Communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SbrWM_mnEDd"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eazCDbmNQqo"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
    "!pip install gym==0.21.0 stable-baselines3==1.8.0 CityLearn==2.1.2 \\\n",
    "    ipywidgets matplotlib seaborn shimmy requests beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import stable_baselines3\n",
    "print(gym.__version__)\n",
    "print(stable_baselines3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEW_ldpKOXDA"
   },
   "source": [
    "We can now import the relevant modules, classes and functions used in the tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVzk4V7qUu2R"
   },
   "outputs": [],
   "source": [
    "# System operations\n",
    "import inspect\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Date and time\n",
    "from datetime import datetime\n",
    "\n",
    "# type hinting\n",
    "from typing import Any, List, Mapping, Tuple, Union\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# User interaction\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import Button, FloatSlider, HBox, HTML\n",
    "from ipywidgets import IntProgress, Text, VBox\n",
    "\n",
    "# Data manipulation\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import simplejson as json\n",
    "\n",
    "# CityLearn\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction\n",
    "from citylearn.wrappers import NormalizedObservationWrapper\n",
    "from citylearn.wrappers import StableBaselines3Wrapper\n",
    "from citylearn.wrappers import TabularQLearningWrapper\n",
    "\n",
    "# baseline RL algorithms\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyCH2neUw4IN"
   },
   "source": [
    "Here we include some global settings we want applied for the remainder of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDAe5ZeVw7_q"
   },
   "outputs": [],
   "source": [
    "# set all plotted figures without margins\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gRR9HOBxOR2"
   },
   "source": [
    "## Loading the Data\n",
    "\n",
    "The dataset is included in the CityLearn library installation which we will now read into memory. To read the dataset, all we need is the `schema.json` that defines it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SuxbmkixQ2z"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2022_phase_all'\n",
    "schema = DataSet.get_schema(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb-xiU9qxUUl"
   },
   "source": [
    "> ⚠️ **NOTE**: To get the names of all datasets in CityLearn execute the `citylearn.data.Dataset.get_names` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8W88P96xXiR"
   },
   "outputs": [],
   "source": [
    "print('All CityLearn datasets:', sorted(DataSet.get_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41IxMp9rxb5W"
   },
   "source": [
    "### Preview a Building Data File\n",
    "We can now preview the data files for one of the buildings in the `citylearn_challenge_2022_phase_all` dataset. The schema includes a `root_directory` key-value where all files that are relevant to this dataset are stored as well as the name of each file. We will use this root directory and filenames to read a building file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0LorchUxg8v"
   },
   "outputs": [],
   "source": [
    "root_directory = schema['root_directory']\n",
    "\n",
    "# change the suffix number in the next code line to a\n",
    "# number between 1 and 17 to preview other buildings\n",
    "building_name = 'Building_1'\n",
    "\n",
    "filename = schema['buildings'][building_name]['energy_simulation']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "building_data = pd.read_csv(filepath)\n",
    "display(building_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWgMhWFjxl-z"
   },
   "source": [
    "This building file has 12 fields describing the hourly building loads and indoor environmental conditions. Descriptive statistics of these fields are reported below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMq9RpWUxoq-"
   },
   "outputs": [],
   "source": [
    "display(building_data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtCpbXiNxvM_"
   },
   "source": [
    "The `Month`, `Hour`, `Day Type` and `Daylight Savings Status` define the temporal dimension of the building loads. `Indoor Temperature [C]`, `Average Unmet Cooling Setpoint Difference [C]` and `Indoor Relative Humidity [%]` are null values in the entire time series since they are not provided in the original dataset from the real-world building. However, they are included in the file to maintain compatibility with the methods used to construct a CityLearn environment. For the same reason, `DHW Heating [kWh]`, `Cooling Load [kWh]` and `Heating Load [kWh]` have zero values throughout the time series as they have coupled with the non-shiftable `Equipment Electric Power [kWh]`.\n",
    "\n",
    "Thus, the goal with this dataset is to learn to use battery-PV system to satisfy these non-shiftable loads. The `Equipment Electric Power [kWh]` and `Solar Generation [W/kW]` time series for the building are plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WieiE91Fx0xr"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 2))\n",
    "x = building_data.index\n",
    "y1 = building_data['Equipment Electric Power [kWh]']\n",
    "y2 = building_data['Solar Generation [W/kW]']\n",
    "axs[0].plot(x, y1)\n",
    "axs[0].set_xlabel('Time step')\n",
    "axs[0].set_ylabel('Equipment Electric Power\\n[kWh]')\n",
    "axs[1].plot(x, y2)\n",
    "axs[1].set_xlabel('Time step')\n",
    "axs[1].set_ylabel('Solar Generation\\n[W/kW]')\n",
    "fig.suptitle(building_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kChHIRCcx7HS"
   },
   "source": [
    "### Preview Weather File\n",
    "\n",
    "Other supplemental data in the dataset include [TMY3 weather data from the Los Angeles International Airport weather station](https://energyplus.net/weather-location/north_and_central_america_wmo_region_4/USA/CA/USA_CA_Los.Angeles.Intl.AP.722950_TMY3) that is representative of a typical meteorological year in the Los Angeles International Airport location. All buildings in this dataset reference the same weather file as previewed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7hovT6HyAfb"
   },
   "outputs": [],
   "source": [
    "filename = schema['buildings'][building_name]['weather']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "weather_data = pd.read_csv(filepath)\n",
    "display(weather_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrVYBj_EyFwE"
   },
   "source": [
    "The weather file has fields that describe the outdoor dry-bulb temperature, relative humidity, diffuse and direct solar radiation, as well as their 6 hour, 12 hour and 24 hour forecasts. In this dataset, the forecasts are perfect forecasts for example, the 6 hour outdoor dry-bulb temperature forecast at a certain time step is equal to the temperature 6 hours later. The summary statistics for the weather fields are provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSuE0q0ayJ9l"
   },
   "outputs": [],
   "source": [
    "display(weather_data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIST0gHIyM90"
   },
   "source": [
    "We can also plot this weather data on an axes to understand it better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwpuHpIUyPpk"
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Outdoor Drybulb Temperature [C]', 'Relative Humidity [%]',\n",
    "    'Diffuse Solar Radiation [W/m2]', 'Direct Solar Radiation [W/m2]'\n",
    "]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18, 5))\n",
    "x = weather_data.index\n",
    "\n",
    "for ax, c in zip(fig.axes, columns):\n",
    "    y = weather_data[c]\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel(c)\n",
    "\n",
    "fig.align_ylabels()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3deLJEwyWNy"
   },
   "source": [
    "### Preview Electricity Price Data\n",
    "\n",
    "The electricity rate-plan for the dataset is that of the community's utility provider, [Southern California Edison](https://www.sce.com/residential/rates/Time-Of-Use-Residential-Rate-Plans). We adopt their _TOU-D-PRIME_ rate plan summarized in table below, which is designed for customers with residential batteries where electricity is cheapest in the early morning and late at night, and cheaper during off-peak months of October-May. Meanwhile, electricity is cheaper on weekends for peak hours of 4 PM-9 PM in June-September.\n",
    "\n",
    "Table: Time-Of-Use rate plan ($/kWh).\n",
    "\n",
    "| | June-September |  | October-May |  |\n",
    "|---|---|---|---|---|\n",
    "| **Time** | **Weekday** | **Weekend** | **Weekday** | **Weekend** |\n",
    "| 8 AM-4 PM | 0.21 | 0.21 | 0.20 | 0.20 |\n",
    "| 4 PM-9 PM | 0.54 | 0.40 | 0.50 | 0.50 |\n",
    "| 9 PM-8 AM | 0.21 | 0.21 | 0.20 | 0.20 |\n",
    "\n",
    "The electricity pricing time series is shown below. It has four fields including perfect forecast of the pricing 6, 12 and 24 hours ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hv2Bc6Qoyr2X"
   },
   "outputs": [],
   "source": [
    "filename = schema['buildings'][building_name]['pricing']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "pricing_data = pd.read_csv(filepath)\n",
    "display(pricing_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKq_lE1MyuTB"
   },
   "source": [
    "### Preview Carbon Intensity Data\n",
    "\n",
    "Another supplementary data in the `citylearn_challenge_2022_phase_all` dataset is the grid carbon intensity time series descring the CO<sub>2</sub> equivalent of greenhouse gases that are emitted for every unit kWh of energy consumption. This carbon intensity data were provided by EPRI and the time series is shown with its summary statistics below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXMSuDcOyv_6"
   },
   "outputs": [],
   "source": [
    "filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "carbon_intensity_data = pd.read_csv(filepath)\n",
    "display(carbon_intensity_data.head())\n",
    "display(carbon_intensity_data.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfFUem5My_QN"
   },
   "source": [
    "We also preview the carbon intensity time series on a pair of axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q771yVRfzAXA"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 2))\n",
    "x = carbon_intensity_data.index\n",
    "y = carbon_intensity_data['kg_CO2/kWh']\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('kg_CO2/kWh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoF-BxSM5Jkc"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now that we are familiar with the CityLearn `citylearn_challenge_2022_phase_all` dataset, we will make minor changes to its schema that will improve our learning experience in this tutorial. These changes are as follows:\n",
    "\n",
    "1. We want to use a subset of the buildings so that we are not overwhelmed by the amount of data to analyze during the tutorial 🙂. Since CityLearn is primarily designed for district level energy management and coordination we should use more than 1 building, although a 1-building environment is possible. A considerable building count for tutoring purposes is 2-3.\n",
    "2. We want to use only a one-week period from the entire one-year period for this tutorial for the same reason of ease of analysis.\n",
    "3. Instead of using the [full observation space](https://www.citylearn.net/overview/observations.html) that will take a while to explore and converge in RL implementations, we will narrow down the space to only one observation: `hour`. This is not the best set up because the hour alone does not explain the state transitions in the environment that the agent is observing, nevertheless, it will help highlight the strengths and weaknesses of different control algorithms.\n",
    "4. CityLearn allows for two control strategies: centralized and decentralized as earlier discussed. In this tutorial we will make use of the former.\n",
    "\n",
    "We will make these modifications directly in the schema. To keep things interesting, the buildings and one-week period will be pseudo-randomly selected but for reproducibility, we will set the random generator seed. This seed can be changed to any value to select another pseudorandom set of buildings and time period. Also, we will provide a method to set the observations we want to use in our simulations so that later down the line, it will be easy to switch and utilize other observations. We will define three functions to help us make these decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o6iEzE_zP_U"
   },
   "outputs": [],
   "source": [
    "def set_schema_buildings(\n",
    "schema: dict, count: int, seed: int\n",
    ") -> Tuple[dict, List[str]]:\n",
    "    \"\"\"Randomly select number of buildings to set as active in the schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    count: int\n",
    "        Number of buildings to set as active in schema.\n",
    "    seed: int\n",
    "        Seed for pseudo-random number generator\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with active buildings set.\n",
    "    buildings: List[str]\n",
    "        List of selected buildings.\n",
    "    \"\"\"\n",
    "\n",
    "    assert 1 <= count <= 15, 'count must be between 1 and 15.'\n",
    "\n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get all building names\n",
    "    buildings = list(schema['buildings'].keys())\n",
    "\n",
    "    # remove buildins 12 and 15 as they have pecularities in their data\n",
    "    # that are not relevant to this tutorial\n",
    "    buildings_to_exclude = ['Building_12', 'Building_15']\n",
    "\n",
    "    for b in buildings_to_exclude:\n",
    "        buildings.remove(b)\n",
    "\n",
    "    # randomly select specified number of buildings\n",
    "    buildings = np.random.choice(buildings, size=count, replace=False).tolist()\n",
    "\n",
    "    # reorder buildings\n",
    "    building_ids = [int(b.split('_')[-1]) for b in buildings]\n",
    "    building_ids = sorted(building_ids)\n",
    "    buildings = [f'Building_{i}' for i in building_ids]\n",
    "\n",
    "    # update schema to only included selected buildings\n",
    "    for b in schema['buildings']:\n",
    "        if b in buildings:\n",
    "            schema['buildings'][b]['include'] = True\n",
    "        else:\n",
    "            schema['buildings'][b]['include'] = False\n",
    "\n",
    "    return schema, buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcfYt2eH0sP8"
   },
   "outputs": [],
   "source": [
    "def set_schema_simulation_period(\n",
    "    schema: dict, count: int, seed: int\n",
    ") -> Tuple[dict, int, int]:\n",
    "    \"\"\"Randomly select environment simulation start and end time steps\n",
    "    that cover a specified number of days.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    count: int\n",
    "        Number of simulation days.\n",
    "    seed: int\n",
    "        Seed for pseudo-random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with `simulation_start_time_step`\n",
    "        and `simulation_end_time_step` key-values set.\n",
    "    simulation_start_time_step: int\n",
    "        The first time step in schema time series files to\n",
    "        be read when constructing the environment.\n",
    "    simulation_end_time_step: int\n",
    "        The last time step in schema time series files to\n",
    "        be read when constructing the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    assert 1 <= count <= 365, 'count must be between 1 and 365.'\n",
    "\n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # use any of the files to determine the total\n",
    "    # number of available time steps\n",
    "    filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "    filepath = os.path.join(root_directory, filename)\n",
    "    time_steps = pd.read_csv(filepath).shape[0]\n",
    "\n",
    "    # set candidate simulation start time steps\n",
    "    # spaced by the number of specified days\n",
    "    simulation_start_time_step_list = np.arange(0, time_steps, 24*count)\n",
    "\n",
    "    # randomly select a simulation start time step\n",
    "    simulation_start_time_step = np.random.choice(\n",
    "        simulation_start_time_step_list, size=1\n",
    "    )[0]\n",
    "    simulation_end_time_step = simulation_start_time_step + 24*count - 1\n",
    "\n",
    "    # update schema simulation time steps\n",
    "    schema['simulation_start_time_step'] = simulation_start_time_step\n",
    "    schema['simulation_end_time_step'] = simulation_end_time_step\n",
    "\n",
    "    return schema, simulation_start_time_step, simulation_end_time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A04LckaH0uuS"
   },
   "outputs": [],
   "source": [
    "def set_active_observations(\n",
    "    schema: dict, active_observations: List[str]\n",
    ") -> dict:\n",
    "    \"\"\"Set the observations that will be part of the environment's\n",
    "    observation space that is provided to the control agent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    active_observations: List[str]\n",
    "        Names of observations to set active to be passed to control agent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with active observations set.\n",
    "    \"\"\"\n",
    "\n",
    "    active_count = 0\n",
    "\n",
    "    for o in schema['observations']:\n",
    "        if o in active_observations:\n",
    "            schema['observations'][o]['active'] = True\n",
    "            active_count += 1\n",
    "        else:\n",
    "            schema['observations'][o]['active'] = False\n",
    "\n",
    "    valid_observations = list(schema['observations'].keys())\n",
    "    assert active_count == len(active_observations),\\\n",
    "        'the provided observations are not all valid observations.'\\\n",
    "          f' Valid observations in CityLearn are: {valid_observations}'\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te2L4gwOzUQo"
   },
   "source": [
    "### Setting your Random Seed\n",
    "\n",
    "Begin by setting a random seed. You can set the seed to any integer including your birth day, month or year. Perhaps lucky number 😁. Choose wisely because we will use this random seed moving forward 😉?!?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfnO0QBszXcS"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 7\n",
    "print('Random seed:', RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX5rxfNdz3lE"
   },
   "source": [
    "### Setting the Buildings, Time Periods and Observations to use in Simulations from the Schema\n",
    "\n",
    "Now we can pseudo-randomly select buildings and time periods as well as set the active observations we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C6S46xmz50t"
   },
   "outputs": [],
   "source": [
    "# edit next code line to change number of buildings in simulation\n",
    "BUILDING_COUNT = 2\n",
    "\n",
    " # edit next code line to change number of days in simulation\n",
    "DAY_COUNT = 7\n",
    "\n",
    "# edit next code line to change active observations in simulation\n",
    "ACTIVE_OBSERVATIONS = ['hour']\n",
    "\n",
    "schema, buildings = set_schema_buildings(schema, BUILDING_COUNT, RANDOM_SEED)\n",
    "schema, simulation_start_time_step, simulation_end_time_step =\\\n",
    "    set_schema_simulation_period(schema, DAY_COUNT, RANDOM_SEED)\n",
    "schema = set_active_observations(schema, ACTIVE_OBSERVATIONS)\n",
    "\n",
    "print('Selected buildings:', buildings)\n",
    "print(\n",
    "    f'Selected {DAY_COUNT}-day period time steps:',\n",
    "    (simulation_start_time_step, simulation_end_time_step)\n",
    ")\n",
    "print(f'Active observations:', ACTIVE_OBSERVATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xL8ayLYz90X"
   },
   "source": [
    "Lastly, the choice between either control strategy is set using the `central_agent` parameter in CityLearn, which is a key-value in the `schema`. We set the `central_agent` key-value in the schema to `True` to define an environment that uses one agent to control many buildings (centralized control strategy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh5FKi6Nopbr"
   },
   "outputs": [],
   "source": [
    "schema['central_agent'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dA40P1O1Ho3"
   },
   "outputs": [],
   "source": [
    "def plot_district_load_profiles(envs: Mapping[str, CityLearnEnv]) -> plt.Figure:\n",
    "    \"\"\"Plots district-level net electricty consumption profile\n",
    "    for different control agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure containing plotted axes.\n",
    "    \"\"\"\n",
    "\n",
    "    figsize = (5.0, 1.5)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    for k, v in envs.items():\n",
    "        y = v.net_electricity_consumption\n",
    "        x = range(len(y))\n",
    "        ax.plot(x, y, label=k)\n",
    "\n",
    "    y = v.net_electricity_consumption_without_storage\n",
    "    ax.plot(x, y, label='Baseline')\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('kWh')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "po_TEviJ1MWQ"
   },
   "source": [
    "> ⚠️ **NOTE**:\n",
    "> You do not need to understand the content of the next code cell where a plotting function is defined.\n",
    "\n",
    "The `plot_battery_soc_profiles` function plots the building-level battery state of charge (SoC) profiles can also be used to compare different control agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unYJBsZB1N-X"
   },
   "outputs": [],
   "source": [
    "def plot_battery_soc_profiles(envs: Mapping[str, CityLearnEnv]) -> plt.Figure:\n",
    "    \"\"\"Plots building-level battery SoC profiles fro different control agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure containing plotted axes.\n",
    "    \"\"\"\n",
    "\n",
    "    building_count = len(list(envs.values())[0].buildings)\n",
    "    column_count_limit = 4\n",
    "    row_count = math.ceil(building_count/column_count_limit)\n",
    "    column_count = min(column_count_limit, building_count)\n",
    "    figsize = (4.0*column_count, 1.75*row_count)\n",
    "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        for k, v in envs.items():\n",
    "            soc = np.array(v.buildings[i].electrical_storage.soc)\n",
    "            capacity = v.buildings[i].electrical_storage.capacity_history[0]\n",
    "            y = soc/capacity\n",
    "            x = range(len(y))\n",
    "            ax.plot(x, y, label=k)\n",
    "\n",
    "        ax.set_title(v.buildings[i].name)\n",
    "        ax.set_xlabel('Time step')\n",
    "        ax.set_ylabel('SoC')\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "\n",
    "        if i == building_count - 1:\n",
    "            ax.legend(\n",
    "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
    "            )\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSBpXJTg1R0s"
   },
   "source": [
    "> ⚠️ **NOTE**:\n",
    "> You do not need to understand the content of the next code cell where a plotting function is defined.\n",
    "\n",
    "The last function, `plot_simulation_summary` is a convenience function used to plot all figures in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyZrdT5a1UJM"
   },
   "outputs": [],
   "source": [
    "def plot_simulation_summary(envs: Mapping[str, CityLearnEnv]):\n",
    "    \"\"\"Plots KPIs, load and battery SoC profiles for different control agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "    \"\"\"\n",
    "\n",
    "    _ = plot_building_kpis(envs)\n",
    "    print('Building-level KPIs:')\n",
    "    plt.show()\n",
    "    _ = plot_building_load_profiles(envs)\n",
    "    print('Building-level load profiles:')\n",
    "    plt.show()\n",
    "    _ = plot_battery_soc_profiles(envs)\n",
    "    print('Battery SoC profiles:')\n",
    "    plt.show()\n",
    "    _ = plot_district_kpis(envs)\n",
    "    print('District-level KPIs:')\n",
    "    plt.show()\n",
    "    print('District-level load profiles:')\n",
    "    _ = plot_district_load_profiles(envs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWXsiZ5freTG"
   },
   "source": [
    "# Build your Custom Rule-Based Controller\n",
    "---\n",
    "\n",
    "With our convenience functions defined, we are ready to start solving our earlier described control problem.\n",
    "\n",
    "We will start simple with a rule-based control (RBC) agent that you will build yourself! RBC is a popular control strategy that is used in most systems e.g. HVAC, batteries, etc because of their level of simplicity. They are  best described as a set of rules expressed as if-else statements and conditions that guide their decision making. An example of such statement is `if outdoor dry-bulb temperature is 20 degrees Celcius and hour 10 PM, charge battery with 5% of capacity`. Now the actual implementation of this statement is open-ended as a designer can choose to program it using any programming language e.g. Python (as used in CityLearn) or a proprietary language that the battery manufacturer uses. Nevertheless, at a high-level, it simplifies to a set of statements and conditions that are easily understood and mappable (think decision tree in supervised learning).\n",
    "\n",
    "The RBC you will be designing here, is a set of if-else statements that use the `hour` observation to determine the amount of energy to charge or discharge a battery. Remember we are using a centralized control strategy thus, the if-else statements you define will apply to all batteries in all buildings.\n",
    "\n",
    "We will use widgets for an interactive RBC tuning experience. You will design a custom RBC that inherits from an existing RBC in CityLearn called the [HourRBC](https://www.citylearn.net/api/citylearn.agents.rbc.html#citylearn.agents.rbc.HourRBC). Inheritance, allows us to copy existing properties and methods in the parent class, `HourRBC`, into our custom class. The `HourRBC` class allows one to define a custom `action_map` using the `hour` as the `if-else` condition and the battery capacity proportion as the `action` where negative proportions imply discharging and positive proportions imply charging.\n",
    "\n",
    "We begin by initializing the environment we will work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unHiw8HH1bzD"
   },
   "outputs": [],
   "source": [
    "rbc_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXyGFcYE1d5j"
   },
   "source": [
    "Now let us define the custom RBC class we will use. All agent classes in CityLearn inherit from the [citylearn.agents.base.Agent](https://www.citylearn.net/api/citylearn.agents.base.html#citylearn.agents.base.Agent) class. This base class has 4 methods that are important to note when defining a new class that inherits from it. namely:\n",
    "\n",
    "1. `__init__` - Used to initialize a new agent with a `citylearn.citylearn.CityLearnEnv` object.\n",
    "2. `learn`: Used to train the initialized object on its environment object.\n",
    "3. `predict`: Used to select actions at each simulation timestep using a defined policy that may be rule-based, reinforcement learning-based or model predictive control-based. The base class selects random actions.\n",
    "4. `update`: Used to update replay buffers, networks and policies at least every timestep. The base class does not perform any updates.\n",
    "5. `next_time_step`: Used to proceed to the next timestep and is called inside `predict`. This function is where class values or custom values that need to collected or updated are best manipulated.\n",
    "\n",
    "In our case with the RBC, we want to include an `action_map` class instance that is a `dict` type. This `action_map` has `int` keys that define hours and `float` values that define charge/discharge action for the hour key that maps them.\n",
    "\n",
    "We also want to include a loader variable to help us visualize the simulation progress. The loader is an `IntProgress` ipywidgets object. We will update the loader's value each timestep the `next_time_step` method is called in the RBC class.\n",
    "\n",
    "The RBC class is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-qNSqYSuiY-"
   },
   "outputs": [],
   "source": [
    "class CustomRBC(HourRBC):\n",
    "   def __init__(\n",
    "       self, env: CityLearnEnv, action_map: Mapping[int, float] = None,\n",
    "       loader: IntProgress = None\n",
    "    ):\n",
    "      r\"\"\"Initialize CustomRBC.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      env: Mapping[str, CityLearnEnv]\n",
    "         CityLearn environment instance.\n",
    "      action_map: Mapping[int, float]\n",
    "         Mapping of hour to control action.\n",
    "      loader: IntProgress\n",
    "         Progress bar.\n",
    "      \"\"\"\n",
    "\n",
    "      super().__init__(env=env, action_map=action_map)\n",
    "      self.loader = loader\n",
    "\n",
    "   def next_time_step(self):\n",
    "      r\"\"\"Advance to next `time_step`.\"\"\"\n",
    "\n",
    "      super().next_time_step()\n",
    "\n",
    "      if self.loader is not None:\n",
    "         self.loader.value += 1\n",
    "      else:\n",
    "         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFj5ovew1uqG"
   },
   "source": [
    "We can now initialize the RBC by setting all actions to 0 for every hour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cr_3VaD1wbe"
   },
   "outputs": [],
   "source": [
    "action_map = {i: 0.0 for i in range(1, 25)}\n",
    "rbc_model = CustomRBC(env=rbc_env, action_map=action_map)\n",
    "print('default RBC action map:', action_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubLZV4k4H0H2"
   },
   "source": [
    "We also need to define a convenience function to set and return a loader i.e. a progress bar as we will use this visualization a number of times to track our learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST3YhSkRIDLT"
   },
   "outputs": [],
   "source": [
    "def get_loader(**kwargs):\n",
    "    \"\"\"Returns a progress bar\"\"\"\n",
    "\n",
    "    kwargs = {\n",
    "        'value': 0,\n",
    "        'min': 0,\n",
    "        'max': 10,\n",
    "        'description': 'Simulating:',\n",
    "        'bar_style': '',\n",
    "        'style': {'bar_color': 'maroon'},\n",
    "        'orientation': 'horizontal',\n",
    "        **kwargs\n",
    "    }\n",
    "    return IntProgress(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIOhohXk12Vo"
   },
   "source": [
    "With our custom RBC now defined, we can set up the interactive widgets.\n",
    "\n",
    "> ⚠️ **NOTE**:\n",
    "> You do not need to understand the content of the next code cell where the widget is defined. Instead wait for the widgets to load and interact with it using the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdmxDMHJuiY_"
   },
   "outputs": [],
   "source": [
    "action_step = 0.05\n",
    "hour_step = 2\n",
    "hours = list(range(1, 25, hour_step))\n",
    "default_loader_description = 'Waiting'\n",
    "questions = \"\"\"\n",
    "<h1>Custom RBC Tuner</h1>\n",
    "<p>Use this interactive widget to tune your custom RBC!\n",
    "Reference the building load profiles above and the questions below when\n",
    "deciding on how to charge/discharge your rule-based controlled batteries.</p>\n",
    "\n",
    "<h3>Some considerations when tuning your custom RBC:</h3>\n",
    "<ul>\n",
    "    <li>What happens when actions for all hours are set to 0?</li>\n",
    "    <li>How can we set the RBC so that it takes advantage\n",
    "    of solar generation?</li>\n",
    "    <li>Can you spot the duck curve?</li>\n",
    "    <li>What settings work best for a specific building?</li>\n",
    "    <li>What settings work best for the entire district?</li>\n",
    "    <li>Can you tune the RBC to target improvements in any one of\n",
    "    the evaluation KPIs?</li>\n",
    "    <li>What challenges can you identify from this RBC tuning process?</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Interact with the controls to tune your RBC:</h3>\n",
    "\n",
    "<p>Use the sliders to set the hourly charge and discharge rate\n",
    "of the batteries. Positive values indicate charging\n",
    "and negative values indicate discharging the batteries</p>\n",
    "\"\"\"\n",
    "html_ui = HTML(value=questions, placeholder='Questions')\n",
    "sliders = [FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-1.0,\n",
    "    max=1.0,\n",
    "    step=action_step,\n",
    "    description=f'Hr: {h}-{h + hour_step - 1}',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='vertical',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ") for h in hours]\n",
    "reset_button = Button(\n",
    "    description='Reset', disabled=False, button_style='info',\n",
    "    tooltip='Set all hour actions to 0.0', icon=''\n",
    ")\n",
    "random_button = Button(\n",
    "    description='Random', disabled=False, button_style='warning',\n",
    "    tooltip='Select random hour actions', icon=''\n",
    ")\n",
    "simulate_button = Button(\n",
    "    description='Simulate', disabled=False, button_style='success',\n",
    "    tooltip='Run simulation', icon='check'\n",
    ")\n",
    "sliders_ui = HBox(sliders)\n",
    "buttons_ui = HBox([reset_button, random_button, simulate_button])\n",
    "\n",
    "# run simulation so that the environment has results\n",
    "# even if user does not interact with widgets\n",
    "sac_episodes = 1\n",
    "rbc_model.learn(episodes=sac_episodes)\n",
    "\n",
    "loader = get_loader(description=default_loader_description)\n",
    "\n",
    "def plot_building_guide(env):\n",
    "    \"\"\"Plots building load and generation profiles.\"\"\"\n",
    "\n",
    "    column_count_limit = 4\n",
    "    building_count = len(env.buildings)\n",
    "    row_count = math.ceil(building_count/column_count_limit)\n",
    "    column_count = min(column_count_limit, building_count)\n",
    "    figsize = (4.0*column_count, 1.5*row_count)\n",
    "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
    "\n",
    "    for i, (ax, b) in enumerate(zip(fig.axes, env.buildings)):\n",
    "        y1 = b.energy_simulation.non_shiftable_load\n",
    "        y2 = b.pv.get_generation(b.energy_simulation.solar_generation)\n",
    "        x = range(len(y1))\n",
    "        ax.plot(x, y1, label='Load')\n",
    "        ax.plot(x, y2, label='Generation')\n",
    "        ax.set_title(b.name)\n",
    "        ax.set_xlabel('Time step')\n",
    "        ax.set_ylabel('kWh')\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "\n",
    "        if i == building_count - 1:\n",
    "            ax.legend(\n",
    "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
    "            )\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def on_reset_button_clicked(b):\n",
    "    \"\"\"Zeros sliders and loader values.\"\"\"\n",
    "\n",
    "    loader.value = 0\n",
    "    loader.description = default_loader_description\n",
    "\n",
    "    for s in sliders:\n",
    "        s.value = 0.0\n",
    "\n",
    "def on_random_button_clicked(b):\n",
    "    \"\"\"Zeros loader value and sets sliders to random values.\"\"\"\n",
    "\n",
    "    loader.value = 0\n",
    "    loader.description = default_loader_description\n",
    "    options = np.arange(-1.0, 1.0, action_step)\n",
    "\n",
    "    for s in sliders:\n",
    "        s.value = round(random.choice(options), 2)\n",
    "\n",
    "def on_simulate_button_clicked(b):\n",
    "    \"\"\"Runs RBC simulation using selected action map.\"\"\"\n",
    "\n",
    "    loader.description = 'Simulating'\n",
    "    loader.value = 0\n",
    "    clear_output(wait=False)\n",
    "\n",
    "    # plot building profiles\n",
    "    _ = plot_building_guide(rbc_env)\n",
    "    plt.show()\n",
    "\n",
    "    display(html_ui, sliders_ui, buttons_ui, loader)\n",
    "    reset_button.disabled = True\n",
    "    random_button.disabled = True\n",
    "    simulate_button.disabled = True\n",
    "\n",
    "    for s in sliders:\n",
    "        s.disabled = True\n",
    "\n",
    "    action_map = {}\n",
    "\n",
    "    for h, s in zip(hours, sliders):\n",
    "        for i in range(hour_step):\n",
    "            action_map[h + i] = s.value\n",
    "\n",
    "    loader.max = rbc_env.time_steps*sac_episodes\n",
    "    rbc_model.action_map = action_map\n",
    "    rbc_model.learn(episodes=sac_episodes)\n",
    "\n",
    "    loader.description = 'Finished'\n",
    "    plot_simulation_summary({'RBC': rbc_env})\n",
    "\n",
    "    reset_button.disabled = False\n",
    "    random_button.disabled = False\n",
    "    simulate_button.disabled = False\n",
    "\n",
    "    for s in sliders:\n",
    "        s.disabled = False\n",
    "\n",
    "reset_button.on_click(on_reset_button_clicked)\n",
    "random_button.on_click(on_random_button_clicked)\n",
    "simulate_button.on_click(on_simulate_button_clicked)\n",
    "\n",
    "# plot building profiles\n",
    "_ = plot_building_guide(rbc_env)\n",
    "plt.show()\n",
    "\n",
    "# preview of building load profile\n",
    "display(html_ui, sliders_ui, buttons_ui, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q74Y_l8a369T"
   },
   "source": [
    "# Optimize a Soft-Actor Critic Reinforcement Learning Controller\n",
    "---\n",
    "\n",
    "To control an environment like CityLearn that has continuous states and actions, tabular Q-learning is not practical, as it suffers from the _curse of dimensionality_. Actor-critic reinforcement learning (RL) methods use artificial neural networks to generalize across the state-action space. The actor network maps the current states to the actions that it estimates to be optimal. Then, the critic network evaluates those actions by mapping them, together with the states under which they were taken, to the Q-values.\n",
    "\n",
    "<figure class=\"image\">\n",
    "  <img src=\"https://github.com/intelligent-environments-lab/CityLearn/blob/master/assets/images/sac_schematic.png?raw=true\"  width=\"350\" alt=\"SAC networks overview.\">\n",
    "  <figcaption>Figure: SAC networks overview (adopted from <a href=\"https://doi.org/10.1145/3408308.3427604\">Vazquez-Canteli et al., 2020</a>).</figcaption>\n",
    "</figure>\n",
    "\n",
    "Soft actor-critic (SAC) is a model-free off-policy RL algorithm. As an off-policy method, SAC can reuse experience and learn from fewer samples. SAC is based on three key elements: an actor-critic architecture, off-policy updates, and entropy maximization for efficient exploration and stable training. SAC learns three different functions: the actor (policy), the critic (soft Q-function), and the value function.\n",
    "\n",
    "This tutorial does not dive into the theory and algorithm of SAC but for interested participants please, refer to [Soft Actor-Critic Algorithms and Applications](https://doi.org/10.48550/arXiv.1812.05905).\n",
    "\n",
    "We will now initialize a new environment and plug it to an SAC agent to help us solve our control problem. Luckily, we do not have to write our own implementation of the SAC algorithm. Instead, we can make use of Python libraries that have standardized the implementation of a number of RL algorithms. One of such libraries that we will use is [Stable Baselines3](https://stable-baselines3.readthedocs.io/en/master/index.html). At the time of writing, there are [13 different RL algorithms](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html#rl-algorithms) implemented between Stable Baselines3 and Stable-Baselines3 - Contrib (contrib package for Stable-Baselines3 - experimental reinforcement learning code), including SAC.\n",
    "\n",
    "The new environment is initialized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9A8-38t390y"
   },
   "outputs": [],
   "source": [
    "sac_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfKkwMPG4Aff"
   },
   "source": [
    "Before our environment is ready for use in Stable Baselines3, we need to take a couple of preprocessing steps in the form of wrappers. Firstly, we will wrap the environment using the `NormalizedObservationWrapper` (see [docs](https://www.citylearn.net/api/citylearn.wrappers.html#citylearn.wrappers.NormalizedObservationWrapper)) that ensure all observations that are served to the agent are [min-max normalized](https://www.codecademy.com/article/normalization) between [0, 1] and cyclical observations e.g. hour, are encoded using the [sine and cosine transformation](https://www.avanwyk.com/encoding-cyclical-features-for-deep-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBH83tFY4DhV"
   },
   "outputs": [],
   "source": [
    "sac_env = NormalizedObservationWrapper(sac_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYyxUFcO4Erv"
   },
   "source": [
    "Next, we wrap with the `StableBaselines3Wrapper` (see [docs](https://www.citylearn.net/api/citylearn.wrappers.html#citylearn.wrappers.StableBaselines3Wrapper)) that ensures observations, actions and rewards are served in manner that is compatible with Stable Baselines3 interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Yq5edYr4JXo"
   },
   "outputs": [],
   "source": [
    "sac_env = StableBaselines3Wrapper(sac_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjx0GjAk4Kux"
   },
   "source": [
    "Now we can go ahead and initialize the SAC model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLkcTNQ34NLy"
   },
   "outputs": [],
   "source": [
    "sac_model = SAC(policy='MlpPolicy', env=sac_env, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HQNnT8O4Qmi"
   },
   "source": [
    "In order to track the progress of learning, we will use a loader as we have done before. Stable Baselines3 makes use of callbacks to help with performing user-defined actions and procedures during learning. However, you do not need to know the specifics of the code below beyond being aware that it is used to update the loader value and store aggregated rewards at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtnL5S394TJB"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env: CityLearnEnv, loader: IntProgress):\n",
    "        r\"\"\"Initialize CustomCallback.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: Mapping[str, CityLearnEnv]\n",
    "            CityLearn environment instance.\n",
    "        loader: IntProgress\n",
    "            Progress bar.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(verbose=0)\n",
    "        self.loader = loader\n",
    "        self.env = env\n",
    "        self.reward_history = [0]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        r\"\"\"Called each time the env step function is called.\"\"\"\n",
    "\n",
    "        if self.env.time_step == 0:\n",
    "            self.reward_history.append(0)\n",
    "\n",
    "        else:\n",
    "            self.reward_history[-1] += sum(self.env.rewards[-1])\n",
    "\n",
    "        self.loader.value += 1\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLSppNHb4ViE"
   },
   "source": [
    "We will train the model for a fraction of the episodes we used to train the Tabular Q-Learning agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hpytx_Rz4onF"
   },
   "outputs": [],
   "source": [
    "# ----------------- CALCULATE NUMBER OF TRAINING EPISODES -----------------\n",
    "fraction = 0.25\n",
    "sac_episodes = int(tql_episodes*fraction)\n",
    "print('Fraction of Tabular Q-Learning episodes used:', fraction)\n",
    "print('Number of episodes to train:', sac_episodes)\n",
    "sac_episode_timesteps = sac_env.time_steps - 1\n",
    "sac_total_timesteps = sac_episodes*sac_episode_timesteps\n",
    "\n",
    "# ------------------------------- SET LOADER ------------------------------\n",
    "sac_loader = get_loader(max=sac_total_timesteps)\n",
    "display(sac_loader)\n",
    "\n",
    "# ------------------------------- TRAIN MODEL -----------------------------\n",
    "sac_callback = CustomCallback(env=sac_env, loader=sac_loader)\n",
    "sac_model = sac_model.learn(\n",
    "    total_timesteps=sac_total_timesteps,\n",
    "    callback=sac_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErTZIqzS4zgO"
   },
   "source": [
    "With the SAC model trained, we will evaluate it for 1 episode using deterministic actions i.e. actions that maximized the Q-values during training as in the Tabular Q-Learning approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SxBBofg5pgL"
   },
   "outputs": [],
   "source": [
    "observations = sac_env.reset()\n",
    "sac_actions_list = []\n",
    "\n",
    "while not sac_env.done:\n",
    "    actions, _ = sac_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = sac_env.step(actions)\n",
    "    sac_actions_list.append(actions)\n",
    "\n",
    "# plot summary and compare with other control results\n",
    "plot_simulation_summary({'RBC': rbc_env, 'TQL': tql_env, 'SAC-1': sac_env})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tk-fLPpx5tZi"
   },
   "source": [
    "<img src=\"https://media.giphy.com/media/80TEu4wOBdPLG/giphy.gif\" height=200></img>\n",
    "\n",
    "The figures show that the SAC agent pretty much did not learn anything! The KPIs remain unchanged compared to the baseline and the battery SoCs are 0 all the time. What might be the case here? Let us have a look a the actions the SAC agent prescribed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLIKfCXO5xiD"
   },
   "outputs": [],
   "source": [
    "def plot_actions(actions_list: List[List[float]], title: str) -> plt.Figure:\n",
    "    \"\"\"Plots action time series for different buildings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actions_list: List[List[float]]\n",
    "        List of actions where each element with index, i,\n",
    "        in list is a list of the actions for different buildings\n",
    "        taken at time step i.\n",
    "    title: str\n",
    "        Plot axes title\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure with plotted axes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 1))\n",
    "    columns = [b.name for b in sac_env.buildings]\n",
    "    plot_data = pd.DataFrame(actions_list, columns=columns)\n",
    "    x = list(range(plot_data.shape[0]))\n",
    "\n",
    "    for c in plot_data.columns:\n",
    "        y = plot_data[c].tolist()\n",
    "        ax.plot(x, y, label=c)\n",
    "\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0)\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel(r'$\\frac{kWh}{kWh_{capacity}}$')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_actions(sac_actions_list, 'SAC-1 Actions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT5LI1iz51jd"
   },
   "source": [
    "<img src=\"https://media.giphy.com/media/b8RfbQFaOs1rO10ren/giphy.gif\" height=200></img>\n",
    "\n",
    "The SAC agent was calling for discharge all the time! To give it away, the reason for this behavior is the reward function that we have used to train the agent 😅.\n",
    "\n",
    "Recall that the Bellman equation uses a reward, $r$, to update the Q-values hence the Q-Table is sensitive to the way the $r$ changes for $(s, a, s')$ tuple. That is to say, we need to make sure the reward we calculate after an action, $a$, is taken at state, $s$, quantifies how-well that action actually causes desirable next state, $s'$. If we define a poor reward function, we risk not learning quickly, or undesirable outcomes. See this example of the [implication of a poorly designed reward function](https://openai.com/research/faulty-reward-functions) where an agent learns to maximize a game score but with dangerous actions!\n",
    "\n",
    "The reward function is a variable in the CityLearn environment. The [docs](https://www.citylearn.net/api/citylearn.reward_function.html) provides information on in-built reward functions that can be used in simulation. The reward function used at run time is that which is defined in the schema and used to construct the environment. It can be overridden by parsing an alternative reward function that inherits from the `citylearn.reward_function.RewardFunction` class (see [docs](https://www.citylearn.net/api/citylearn.reward_function.html#citylearn.reward_function.RewardFunction)). Let us see what the current reward is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57tLgSp858N3"
   },
   "outputs": [],
   "source": [
    "help(sac_env.reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWprvdo46EYE"
   },
   "source": [
    "The current reward functions is the electricity consumption from the grid at the current time step returned as a negative value. While this reward will penalize high electricity consumption, it might not be ideal for all KPIs we are trying to optimize. As you would imagine, the best way to minimize electricity consumption is to try to move all loads to the battery hence, the insistence of the agent to continue to discharge the batteries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dt77XG7I6Fd6"
   },
   "source": [
    "## Defining a Custom Reward Function\n",
    "\n",
    "We want to reduce electricity consumption but also reduce its cost and emissions. Likewise, we want to reduce the peaks and ramping, and increase the load factor. One way to achieve this is to teach the agent to charge the batteries when electricity is cheap after 9 PM and before 4 PM, which typically coincides with when the grid is cleaner (lower emissions). But recall that each building is able to generate power provided there is solar radiation. So, we can take advantage of self-generation in the late morning to late afternoon to charge for free and discharge the rest of the day thus reducing electricity consumption, cost and emissions at the very least. Also, by shifting the early morning and evening peak loads to the batteries we can improve on our peak and load-factor KPIs.\n",
    "\n",
    "We should also teach our agent to ensure that renewable solar generation is not wasted by making use of the PV to charge the batteries while they are charged below capacity. On the flip side, the agent should learn to discharge when there is net positive grid load and the batteries still have stored energy.\n",
    "\n",
    "Given these learning objectives, we can now define a reward function that closely satisfies the criteria for which the agent will learn good rewards:\n",
    "\n",
    "$$\n",
    "    r = \\sum_{i=0}^n \\Big(p_i \\times |C_i|\\Big)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    p_i = -\\left(1 + \\textrm{sign}(C_i) \\times \\textrm{SOC}^{\\textrm{battery}}_i\\right)\n",
    "$$\n",
    "\n",
    "The reward function, $r$, is designed to minimize electricity cost, $C$. It is calculated for each building, $i$ and summed to provide the agent with a reward that is representative of all $n$ buildings. It encourages net-zero energy use by penalizing grid load satisfaction when there is energy in the battery as well as penalizing net export when the battery is not fully charged through the penalty term, $p$. There is neither penalty nor reward when the battery is fully charged during net export to the grid. Whereas, when the battery is charged to capacity and there is net import from the grid the penalty is maximized.\n",
    "\n",
    "Now we define this custom reward below and set it as the reward for the SAC agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPK08TkI6Jsi"
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Union\n",
    "from citylearn.reward_function import RewardFunction\n",
    "\n",
    "class CustomReward(RewardFunction):\n",
    "    r\"\"\"Initialize CustomReward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env_metadata: Mapping[str, Any]\n",
    "        General static information about the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_metadata: Mapping[str, Any]):\n",
    "        super().__init__(env_metadata)\n",
    "\n",
    "    def calculate(\n",
    "        self, observations: List[Mapping[str, Union[int, float]]]\n",
    "    ) -> List[float]:\n",
    "        r\"\"\"Returns reward for the most recent action.\n",
    "\n",
    "        The reward is designed to minimize electricity cost.\n",
    "        It is calculated for each building, *i*, and summed to provide the agent\n",
    "        with a reward that is representative of all *n* buildings.\n",
    "        It encourages net-zero energy use by penalizing grid load satisfaction\n",
    "        when there is energy in the battery, as well as penalizing\n",
    "        net export when the battery is not fully charged (through the penalty\n",
    "        term). There is neither penalty nor reward when the battery\n",
    "        is fully charged during net export to the grid. Whereas, when the\n",
    "        battery is charged to capacity and there is net import from the\n",
    "        grid, the penalty is maximized.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations: List[Mapping[str, Union[int, float]]]\n",
    "            List of all building observations at the current \n",
    "            :py:attr:`citylearn.citylearn.CityLearnEnv.time_step`\n",
    "            (obtained by calling :py:meth:`citylearn.building.Building.observations`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward: List[float]\n",
    "            Reward for the transition to the current timestep.\n",
    "        \"\"\"\n",
    "        reward_list = []\n",
    "\n",
    "        for o, m in zip(observations, self.env_metadata['buildings']):\n",
    "            cost = o['net_electricity_consumption'] * o['electricity_pricing']\n",
    "            battery_capacity = m['electrical_storage']['capacity']\n",
    "            battery_soc = o.get('electrical_storage_soc', 0.0)\n",
    "            penalty = -(1.0 + np.sign(cost) * battery_soc)\n",
    "            reward = penalty * abs(cost)\n",
    "            reward_list.append(reward)\n",
    "\n",
    "        reward = [sum(reward_list)]\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lolPaXje6Mlk"
   },
   "source": [
    "Let us repeat all the previous steps we took in the former SAC simulation where the only difference in the workflow here is the use of our new custom reward function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38i9ZAnp6Ns7"
   },
   "outputs": [],
   "source": [
    "# ----------------- INITIALIZE ENVIRONMENT -----------------\n",
    "sacr_env = CityLearnEnv(schema)\n",
    "\n",
    "# -------------------- SET CUSTOM REWARD -------------------\n",
    "sacr_env.reward_function = CustomReward(sacr_env.get_metadata())\n",
    "\n",
    "# -------------------- WRAP ENVIRONMENT --------------------\n",
    "sacr_env = NormalizedObservationWrapper(sacr_env)\n",
    "sacr_env = StableBaselines3Wrapper(sacr_env)\n",
    "\n",
    "# -------------------- INITIALIZE AGENT --------------------\n",
    "sacr_model = SAC(policy='MlpPolicy', env=sacr_env, seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ----------------------- SET LOADER -----------------------\n",
    "print('Number of episodes to train:', sac_episodes)\n",
    "sac_modr_loader = get_loader(max=sac_total_timesteps)\n",
    "display(sac_modr_loader)\n",
    "\n",
    "# ----------------------- TRAIN AGENT ----------------------\n",
    "sacr_callback = CustomCallback(env=sacr_env, loader=sac_modr_loader)\n",
    "sacr_model = sacr_model.learn(\n",
    "    total_timesteps=sac_total_timesteps,\n",
    "    callback=sacr_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsMNFhJX6RWb"
   },
   "source": [
    "Finally, evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9uB1Fr56TmN"
   },
   "outputs": [],
   "source": [
    "observations = sacr_env.reset()\n",
    "sacr_actions_list = []\n",
    "\n",
    "while not sacr_env.done:\n",
    "    actions, _ = sacr_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = sacr_env.step(actions)\n",
    "    sacr_actions_list.append(actions)\n",
    "\n",
    "plot_simulation_summary(\n",
    "    {'RBC': rbc_env, 'TQL': tql_env, 'SAC-1': sac_env, 'SAC-2': sacr_env}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46jpY4Or6X4I"
   },
   "source": [
    "Finally, we have results that have improved the baseline KPIs all thanks to our custom reward function! The agent has learned to take advantage of the solar generation to charge the batteries and discharge the stored energy during the evening peak.\n",
    "\n",
    "Let us now have a look at the actions that the agent predicted in the deterministic simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s2C5gOf6aSO"
   },
   "outputs": [],
   "source": [
    "fig = plot_actions(sacr_actions_list, 'SAC Actions using Custom Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geSmAWJ76ePi"
   },
   "source": [
    "The agent learned the different building needs as building 7 begins to charge later than building 2 daily (selected buildings when `RANDOM_SEED` = 0). The agent discharges the batteries differently as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJniwtkc6gz9"
   },
   "source": [
    "## Evaluate the Episode Rewards for RL Algorithms\n",
    "\n",
    "We can also investigate the convergence rate in training by looking at the sum of rewards in each episode. We expect to see the reward sum increase as we train on more episodes and eventually plateau when exploitation increases or performance can not be further improved. We will look at the reward trajectory for the Tabular Q-Learning, SAC with and without custom reward models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D-Qlfv-6kNI"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ax: plt.Axes, rewards: List[float], title: str) -> plt.Axes:\n",
    "    \"\"\"Plots rewards over training episodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards: List[float]\n",
    "        List of reward sum per episode.\n",
    "    title: str\n",
    "        Plot axes title\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: plt.Axes\n",
    "        Plotted axes\n",
    "    \"\"\"\n",
    "\n",
    "    ax.plot(rewards)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZOaEMwQ6lqa"
   },
   "outputs": [],
   "source": [
    "rewards = {\n",
    "    'Tabular Q-Learning': tql_model.reward_history[:tql_episodes],\n",
    "    'SAC-1': sac_callback.reward_history[:sac_episodes],\n",
    "    'SAC-2': sacr_callback.reward_history[:sac_episodes]\n",
    "}\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 2))\n",
    "\n",
    "for ax, (k, v) in zip(fig.axes, rewards.items()):\n",
    "    ax = plot_rewards(ax, v, k)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "vscode": {
   "interpreter": {
    "hash": "aef885a76ce31739e452d1e6967b400907b14827afd25732d0e38ec88d4e0d05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13i7KQ9t-CV8"
   },
   "source": [
    "# CityLearn: A Tutorial on Reinforcement Learning Control for Grid-Interactive Efficient Buildings and Communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SbrWM_mnEDd"
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eazCDbmNQqo"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
    "!pip install gym==0.21.0 stable-baselines3==1.8.0 CityLearn==2.1.2 \\\n",
    "    ipywidgets matplotlib seaborn shimmy requests beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import stable_baselines3\n",
    "print(gym.__version__)\n",
    "print(stable_baselines3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVzk4V7qUu2R"
   },
   "outputs": [],
   "source": [
    "# System operations\n",
    "import inspect\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "# Date and time\n",
    "from datetime import datetime\n",
    "\n",
    "# type hinting\n",
    "from typing import Any, List, Mapping, Tuple, Union\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# User interaction\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import Button, FloatSlider, HBox, HTML\n",
    "from ipywidgets import IntProgress, Text, VBox\n",
    "\n",
    "# Data manipulation\n",
    "from bs4 import BeautifulSoup\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import simplejson as json\n",
    "\n",
    "# CityLearn\n",
    "from citylearn.agents.rbc import HourRBC\n",
    "from citylearn.agents.q_learning import TabularQLearning\n",
    "from citylearn.citylearn import CityLearnEnv\n",
    "from citylearn.data import DataSet\n",
    "from citylearn.reward_function import RewardFunction\n",
    "from citylearn.wrappers import NormalizedObservationWrapper\n",
    "from citylearn.wrappers import StableBaselines3Wrapper\n",
    "from citylearn.wrappers import TabularQLearningWrapper\n",
    "\n",
    "# baseline RL algorithms\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDAe5ZeVw7_q"
   },
   "outputs": [],
   "source": [
    "# set all plotted figures without margins\n",
    "plt.rcParams['axes.xmargin'] = 0\n",
    "plt.rcParams['axes.ymargin'] = 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9SuxbmkixQ2z"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = 'citylearn_challenge_2022_phase_all'\n",
    "schema = DataSet.get_schema(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8W88P96xXiR"
   },
   "outputs": [],
   "source": [
    "print('All CityLearn datasets:', sorted(DataSet.get_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0LorchUxg8v"
   },
   "outputs": [],
   "source": [
    "root_directory = schema['root_directory']\n",
    "\n",
    "# change the suffix number in the next code line to a\n",
    "# number between 1 and 17 to preview other buildings\n",
    "building_name = 'Building_1'\n",
    "\n",
    "filename = schema['buildings'][building_name]['energy_simulation']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "building_data = pd.read_csv(filepath)\n",
    "display(building_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMq9RpWUxoq-"
   },
   "outputs": [],
   "source": [
    "display(building_data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WieiE91Fx0xr"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(18, 2))\n",
    "x = building_data.index\n",
    "y1 = building_data['Equipment Electric Power [kWh]']\n",
    "y2 = building_data['Solar Generation [W/kW]']\n",
    "axs[0].plot(x, y1)\n",
    "axs[0].set_xlabel('Time step')\n",
    "axs[0].set_ylabel('Equipment Electric Power\\n[kWh]')\n",
    "axs[1].plot(x, y2)\n",
    "axs[1].set_xlabel('Time step')\n",
    "axs[1].set_ylabel('Solar Generation\\n[W/kW]')\n",
    "fig.suptitle(building_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x7hovT6HyAfb"
   },
   "outputs": [],
   "source": [
    "filename = schema['buildings'][building_name]['weather']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "weather_data = pd.read_csv(filepath)\n",
    "display(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSuE0q0ayJ9l"
   },
   "outputs": [],
   "source": [
    "display(weather_data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwpuHpIUyPpk"
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'Outdoor Drybulb Temperature [C]', 'Relative Humidity [%]',\n",
    "    'Diffuse Solar Radiation [W/m2]', 'Direct Solar Radiation [W/m2]'\n",
    "]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18, 5))\n",
    "x = weather_data.index\n",
    "\n",
    "for ax, c in zip(fig.axes, columns):\n",
    "    y = weather_data[c]\n",
    "    ax.plot(x, y)\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel(c)\n",
    "\n",
    "fig.align_ylabels()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hv2Bc6Qoyr2X"
   },
   "outputs": [],
   "source": [
    "filename = schema['buildings'][building_name]['pricing']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "pricing_data = pd.read_csv(filepath)\n",
    "display(pricing_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXMSuDcOyv_6"
   },
   "outputs": [],
   "source": [
    "filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "filepath = os.path.join(root_directory, filename)\n",
    "carbon_intensity_data = pd.read_csv(filepath)\n",
    "display(carbon_intensity_data.head())\n",
    "display(carbon_intensity_data.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q771yVRfzAXA"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(9, 2))\n",
    "x = carbon_intensity_data.index\n",
    "y = carbon_intensity_data['kg_CO2/kWh']\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel('Time step')\n",
    "ax.set_ylabel('kg_CO2/kWh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o6iEzE_zP_U"
   },
   "outputs": [],
   "source": [
    "def set_schema_buildings(\n",
    "schema: dict, count: int, seed: int\n",
    ") -> Tuple[dict, List[str]]:\n",
    "    \"\"\"Randomly select number of buildings to set as active in the schema.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    count: int\n",
    "        Number of buildings to set as active in schema.\n",
    "    seed: int\n",
    "        Seed for pseudo-random number generator\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with active buildings set.\n",
    "    buildings: List[str]\n",
    "        List of selected buildings.\n",
    "    \"\"\"\n",
    "\n",
    "    assert 1 <= count <= 15, 'count must be between 1 and 15.'\n",
    "\n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # get all building names\n",
    "    buildings = list(schema['buildings'].keys())\n",
    "\n",
    "    # remove buildins 12 and 15 as they have pecularities in their data\n",
    "    # that are not relevant to this tutorial\n",
    "    buildings_to_exclude = ['Building_12', 'Building_15']\n",
    "\n",
    "    for b in buildings_to_exclude:\n",
    "        buildings.remove(b)\n",
    "\n",
    "    # randomly select specified number of buildings\n",
    "    buildings = np.random.choice(buildings, size=count, replace=False).tolist()\n",
    "\n",
    "    # reorder buildings\n",
    "    building_ids = [int(b.split('_')[-1]) for b in buildings]\n",
    "    building_ids = sorted(building_ids)\n",
    "    buildings = [f'Building_{i}' for i in building_ids]\n",
    "\n",
    "    # update schema to only included selected buildings\n",
    "    for b in schema['buildings']:\n",
    "        if b in buildings:\n",
    "            schema['buildings'][b]['include'] = True\n",
    "        else:\n",
    "            schema['buildings'][b]['include'] = False\n",
    "\n",
    "    return schema, buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcfYt2eH0sP8"
   },
   "outputs": [],
   "source": [
    "def set_schema_simulation_period(\n",
    "    schema: dict, count: int, seed: int\n",
    ") -> Tuple[dict, int, int]:\n",
    "    \"\"\"Randomly select environment simulation start and end time steps\n",
    "    that cover a specified number of days.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    count: int\n",
    "        Number of simulation days.\n",
    "    seed: int\n",
    "        Seed for pseudo-random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with `simulation_start_time_step`\n",
    "        and `simulation_end_time_step` key-values set.\n",
    "    simulation_start_time_step: int\n",
    "        The first time step in schema time series files to\n",
    "        be read when constructing the environment.\n",
    "    simulation_end_time_step: int\n",
    "        The last time step in schema time series files to\n",
    "        be read when constructing the environment.\n",
    "    \"\"\"\n",
    "\n",
    "    assert 1 <= count <= 365, 'count must be between 1 and 365.'\n",
    "\n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # use any of the files to determine the total\n",
    "    # number of available time steps\n",
    "    filename = schema['buildings'][building_name]['carbon_intensity']\n",
    "    filepath = os.path.join(root_directory, filename)\n",
    "    time_steps = pd.read_csv(filepath).shape[0]\n",
    "\n",
    "    # set candidate simulation start time steps\n",
    "    # spaced by the number of specified days\n",
    "    simulation_start_time_step_list = np.arange(0, time_steps, 24*count)\n",
    "\n",
    "    # randomly select a simulation start time step\n",
    "    simulation_start_time_step = np.random.choice(\n",
    "        simulation_start_time_step_list, size=1\n",
    "    )[0]\n",
    "    simulation_end_time_step = simulation_start_time_step + 24*count - 1\n",
    "\n",
    "    # update schema simulation time steps\n",
    "    schema['simulation_start_time_step'] = simulation_start_time_step\n",
    "    schema['simulation_end_time_step'] = simulation_end_time_step\n",
    "\n",
    "    return schema, simulation_start_time_step, simulation_end_time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A04LckaH0uuS"
   },
   "outputs": [],
   "source": [
    "def set_active_observations(\n",
    "    schema: dict, active_observations: List[str]\n",
    ") -> dict:\n",
    "    \"\"\"Set the observations that will be part of the environment's\n",
    "    observation space that is provided to the control agent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping used to construct environment.\n",
    "    active_observations: List[str]\n",
    "        Names of observations to set active to be passed to control agent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    schema: dict\n",
    "        CityLearn dataset mapping with active observations set.\n",
    "    \"\"\"\n",
    "\n",
    "    active_count = 0\n",
    "\n",
    "    for o in schema['observations']:\n",
    "        if o in active_observations:\n",
    "            schema['observations'][o]['active'] = True\n",
    "            active_count += 1\n",
    "        else:\n",
    "            schema['observations'][o]['active'] = False\n",
    "\n",
    "    valid_observations = list(schema['observations'].keys())\n",
    "    assert active_count == len(active_observations),\\\n",
    "        'the provided observations are not all valid observations.'\\\n",
    "          f' Valid observations in CityLearn are: {valid_observations}'\n",
    "\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "te2L4gwOzUQo"
   },
   "source": [
    "# RANDOM SEED SETTINGS\n",
    "\n",
    "Begin by setting a random seed. You can set the seed to any integer including your birth day, month or year. Perhaps lucky number ðŸ˜. Choose wisely because we will use this random seed moving forward ðŸ˜‰?!?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfnO0QBszXcS"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 26082003\n",
    "print('Random seed:', RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C6S46xmz50t"
   },
   "outputs": [],
   "source": [
    "# edit next code line to change number of buildings in simulation\n",
    "BUILDING_COUNT = 2\n",
    "\n",
    " # edit next code line to change number of days in simulation\n",
    "DAY_COUNT = 7\n",
    "\n",
    "# edit next code line to change active observations in simulation\n",
    "ACTIVE_OBSERVATIONS = ['hour']\n",
    "\n",
    "schema, buildings = set_schema_buildings(schema, BUILDING_COUNT, RANDOM_SEED)\n",
    "schema, simulation_start_time_step, simulation_end_time_step =\\\n",
    "    set_schema_simulation_period(schema, DAY_COUNT, RANDOM_SEED)\n",
    "schema = set_active_observations(schema, ACTIVE_OBSERVATIONS)\n",
    "\n",
    "print('Selected buildings:', buildings)\n",
    "print(\n",
    "    f'Selected {DAY_COUNT}-day period time steps:',\n",
    "    (simulation_start_time_step, simulation_end_time_step)\n",
    ")\n",
    "print(f'Active observations:', ACTIVE_OBSERVATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qh5FKi6Nopbr"
   },
   "outputs": [],
   "source": [
    "schema['central_agent'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSt6h_Q-oqjK"
   },
   "source": [
    "# Initialize a CityLearn Environment\n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aBJ5aLZosk-"
   },
   "outputs": [],
   "source": [
    "env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYT8ouXg0WeH"
   },
   "outputs": [],
   "source": [
    "print('Current time step:', env.time_step)\n",
    "print('environment number of time steps:', env.time_steps)\n",
    "print('environment uses central agent:', env.central_agent)\n",
    "print('Number of buildings:', len(env.buildings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gT64404w0dlu"
   },
   "outputs": [],
   "source": [
    "# electrical storage\n",
    "print('Electrical storage capacity:', {\n",
    "    b.name: b.electrical_storage.capacity for b in env.buildings\n",
    "})\n",
    "print('Electrical storage nominal power:', {\n",
    "    b.name: b.electrical_storage.nominal_power for b in env.buildings\n",
    "})\n",
    "print('Electrical storage capacity history:', {\n",
    "    b.name: b.electrical_storage.capacity_history[b.time_step] for b in env.buildings\n",
    "})\n",
    "print('Electrical storage loss_coefficient:', {\n",
    "    b.name: b.electrical_storage.loss_coefficient for b in env.buildings\n",
    "})\n",
    "print('Electrical storage initial_soc:', {\n",
    "    b.name: b.electrical_storage.initial_soc for b in env.buildings\n",
    "})\n",
    "print('Electrical storage soc:', {\n",
    "    b.name: b.electrical_storage.soc[b.time_step] for b in env.buildings\n",
    "})\n",
    "print('Electrical storage efficiency:', {\n",
    "    b.name: b.electrical_storage.efficiency for b in env.buildings\n",
    "})\n",
    "print('Electrical storage efficiency history:', {\n",
    "    b.name: b.electrical_storage.efficiency_history[b.time_step] for b in env.buildings\n",
    "})\n",
    "print('Electrical storage electricity consumption:', {\n",
    "    b.name: b.electrical_storage.electricity_consumption[b.time_step]\n",
    "    for b in env.buildings\n",
    "})\n",
    "print('Electrical storage capacity loss coefficient:', {\n",
    "    b.name: b.electrical_storage.capacity_loss_coefficient for b in env.buildings\n",
    "})\n",
    "print()\n",
    "# pv\n",
    "print('PV nominal power:', {\n",
    "    b.name: b.pv.nominal_power for b in env.buildings\n",
    "})\n",
    "print()\n",
    "\n",
    "# active observations and actions\n",
    "with pd.option_context(\n",
    "    'display.max_rows', None,\n",
    "    'display.max_columns', None,\n",
    "    'display.width', None\n",
    "):\n",
    "    print('Active observations:')\n",
    "    display(pd.DataFrame([\n",
    "        {**{'building':b.name}, **b.observation_metadata}\n",
    "        for b in env.buildings\n",
    "    ]))\n",
    "    print()\n",
    "    print('Active actions:')\n",
    "    display(pd.DataFrame([\n",
    "        {**{'building':b.name}, **b.action_metadata}\n",
    "        for b in env.buildings\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aG2SsVYq0sye"
   },
   "outputs": [],
   "source": [
    "def get_kpis(env: CityLearnEnv) -> pd.DataFrame:\n",
    "    \"\"\"Returns evaluation KPIs.\n",
    "\n",
    "    Electricity consumption, cost and carbon emissions KPIs are provided\n",
    "    at the building-level and average district-level. Average daily peak,\n",
    "    ramping and (1 - load factor) KPIs are provided at the district level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: CityLearnEnv\n",
    "        CityLearn environment instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kpis: pd.DataFrame\n",
    "        KPI table.\n",
    "    \"\"\"\n",
    "\n",
    "    kpis = env.evaluate()\n",
    "\n",
    "    # names of KPIs to retrieve from evaluate function\n",
    "    kpi_names = {\n",
    "        'electricity_consumption_total': 'Consumption',\n",
    "        'cost_total': 'Cost',\n",
    "        'carbon_emissions_total': 'Emissions',\n",
    "        'daily_peak_average': 'Avg. daily peak',\n",
    "        'ramping_average': 'Ramping',\n",
    "        'monthly_one_minus_load_factor_average': '1 - load factor'\n",
    "    }\n",
    "    kpis = kpis[\n",
    "        (kpis['cost_function'].isin(kpi_names))\n",
    "    ].dropna()\n",
    "    kpis['cost_function'] = kpis['cost_function'].map(lambda x: kpi_names[x])\n",
    "\n",
    "    # round up the values to 3 decimal places for readability\n",
    "    kpis['value'] = kpis['value'].round(3)\n",
    "\n",
    "    # rename the column that defines the KPIs\n",
    "    kpis = kpis.rename(columns={'cost_function': 'kpi'})\n",
    "\n",
    "    return kpis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVPVuSdL0xOu"
   },
   "outputs": [],
   "source": [
    "def plot_building_kpis(envs: Mapping[str, CityLearnEnv]) -> plt.Figure:\n",
    "    \"\"\"Plots electricity consumption, cost and carbon emissions\n",
    "    at the building-level for different control agents in bar charts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure containing plotted axes.\n",
    "    \"\"\"\n",
    "\n",
    "    kpis_list = []\n",
    "\n",
    "    for k, v in envs.items():\n",
    "        kpis = get_kpis(v)\n",
    "        kpis = kpis[kpis['level']=='building'].copy()\n",
    "        kpis['building_id'] = kpis['name'].str.split('_', expand=True)[1]\n",
    "        kpis['building_id'] = kpis['building_id'].astype(int).astype(str)\n",
    "        kpis['env_id'] = k\n",
    "        kpis_list.append(kpis)\n",
    "\n",
    "    kpis = pd.concat(kpis_list, ignore_index=True, sort=False)\n",
    "    kpi_names= kpis['kpi'].unique()\n",
    "    column_count_limit = 3\n",
    "    row_count = math.ceil(len(kpi_names)/column_count_limit)\n",
    "    column_count = min(column_count_limit, len(kpi_names))\n",
    "    building_count = len(kpis['name'].unique())\n",
    "    env_count = len(envs)\n",
    "    figsize = (3.0*column_count, 0.3*env_count*building_count*row_count)\n",
    "    fig, _ = plt.subplots(\n",
    "        row_count, column_count, figsize=figsize, sharey=True\n",
    "    )\n",
    "\n",
    "    for i, (ax, (k, k_data)) in enumerate(zip(fig.axes, kpis.groupby('kpi'))):\n",
    "        sns.barplot(x='value', y='name', data=k_data, hue='env_id', ax=ax)\n",
    "        ax.axvline(1.0, color='black', linestyle='--', label='Baseline')\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_title(k)\n",
    "\n",
    "        if i == len(kpi_names) - 1:\n",
    "            ax.legend(\n",
    "                loc='upper left', bbox_to_anchor=(1.3, 1.0), framealpha=0.0\n",
    "            )\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "        for s in ['right','top']:\n",
    "            ax.spines[s].set_visible(False)\n",
    "\n",
    "        for p in ax.patches:\n",
    "            ax.text(\n",
    "                p.get_x() + p.get_width(),\n",
    "                p.get_y() + p.get_height()/2.0,\n",
    "                p.get_width(), ha='left', va='center'\n",
    "            )\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXlXXqtI04Cm"
   },
   "outputs": [],
   "source": [
    "def plot_district_kpis(envs: Mapping[str, CityLearnEnv]) -> plt.Figure:\n",
    "    \"\"\"Plots electricity consumption, cost, carbon emissions,\n",
    "    average daily peak, ramping and (1 - load factor) at the\n",
    "    district-level for different control agents in a bar chart.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure containing plotted axes.\n",
    "    \"\"\"\n",
    "\n",
    "    kpis_list = []\n",
    "\n",
    "    for k, v in envs.items():\n",
    "        kpis = get_kpis(v)\n",
    "        kpis = kpis[kpis['level']=='district'].copy()\n",
    "        kpis['env_id'] = k\n",
    "        kpis_list.append(kpis)\n",
    "\n",
    "    kpis = pd.concat(kpis_list, ignore_index=True, sort=False)\n",
    "    row_count = 1\n",
    "    column_count = 1\n",
    "    env_count = len(envs)\n",
    "    kpi_count = len(kpis['kpi'].unique())\n",
    "    figsize = (6.0*column_count, 0.225*env_count*kpi_count*row_count)\n",
    "    fig, ax = plt.subplots(row_count, column_count, figsize=figsize)\n",
    "    sns.barplot(x='value', y='kpi', data=kpis, hue='env_id', ax=ax)\n",
    "    ax.axvline(1.0, color='black', linestyle='--', label='Baseline')\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(None)\n",
    "\n",
    "    for s in ['right','top']:\n",
    "        ax.spines[s].set_visible(False)\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width(),\n",
    "            p.get_y() + p.get_height()/2.0,\n",
    "            p.get_width(), ha='left', va='center'\n",
    "        )\n",
    "\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.3, 1.0), framealpha=0.0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roq__q_M0_Yx"
   },
   "outputs": [],
   "source": [
    "def plot_building_load_profiles(envs: Mapping[str, CityLearnEnv]) -> plt.Figure:\n",
    "    \"\"\"Plots building-level net electricty consumption profile\n",
    "    for different control agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure containing plotted axes.\n",
    "    \"\"\"\n",
    "\n",
    "    building_count = len(list(envs.values())[0].buildings)\n",
    "    column_count_limit = 4\n",
    "    row_count = math.ceil(building_count/column_count_limit)\n",
    "    column_count = min(column_count_limit, building_count)\n",
    "    figsize = (4.0*column_count, 1.75*row_count)\n",
    "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        for k, v in envs.items():\n",
    "            y = v.buildings[i].net_electricity_consumption\n",
    "            x = range(len(y))\n",
    "            ax.plot(x, y, label=k)\n",
    "\n",
    "        y = v.buildings[i].net_electricity_consumption_without_storage\n",
    "        ax.plot(x, y, label='Baseline')\n",
    "        ax.set_title(v.buildings[i].name)\n",
    "        ax.set_xlabel('Time step')\n",
    "        ax.set_ylabel('kWh')\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "\n",
    "        if i == building_count - 1:\n",
    "            ax.legend(\n",
    "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
    "            )\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dA40P1O1Ho3"
   },
   "outputs": [],
   "source": [
    "def plot_district_load_profiles(envs: Mapping[str, CityLearnEnv]) -> plt.Figure:\n",
    "    \"\"\"Plots district-level net electricty consumption profile\n",
    "    for different control agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure containing plotted axes.\n",
    "    \"\"\"\n",
    "\n",
    "    figsize = (5.0, 1.5)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "\n",
    "    for k, v in envs.items():\n",
    "        y = v.net_electricity_consumption\n",
    "        x = range(len(y))\n",
    "        ax.plot(x, y, label=k)\n",
    "\n",
    "    y = v.net_electricity_consumption_without_storage\n",
    "    ax.plot(x, y, label='Baseline')\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel('kWh')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unYJBsZB1N-X"
   },
   "outputs": [],
   "source": [
    "def plot_battery_soc_profiles(envs: Mapping[str, CityLearnEnv]) -> plt.Figure:\n",
    "    \"\"\"Plots building-level battery SoC profiles fro different control agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure containing plotted axes.\n",
    "    \"\"\"\n",
    "\n",
    "    building_count = len(list(envs.values())[0].buildings)\n",
    "    column_count_limit = 4\n",
    "    row_count = math.ceil(building_count/column_count_limit)\n",
    "    column_count = min(column_count_limit, building_count)\n",
    "    figsize = (4.0*column_count, 1.75*row_count)\n",
    "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        for k, v in envs.items():\n",
    "            soc = np.array(v.buildings[i].electrical_storage.soc)\n",
    "            capacity = v.buildings[i].electrical_storage.capacity_history[0]\n",
    "            y = soc/capacity\n",
    "            x = range(len(y))\n",
    "            ax.plot(x, y, label=k)\n",
    "\n",
    "        ax.set_title(v.buildings[i].name)\n",
    "        ax.set_xlabel('Time step')\n",
    "        ax.set_ylabel('SoC')\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "\n",
    "        if i == building_count - 1:\n",
    "            ax.legend(\n",
    "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
    "            )\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyZrdT5a1UJM"
   },
   "outputs": [],
   "source": [
    "def plot_simulation_summary(envs: Mapping[str, CityLearnEnv]):\n",
    "    \"\"\"Plots KPIs, load and battery SoC profiles for different control agents.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    envs: Mapping[str, CityLearnEnv]\n",
    "        Mapping of user-defined control agent names to environments\n",
    "        the agents have been used to control.\n",
    "    \"\"\"\n",
    "\n",
    "    _ = plot_building_kpis(envs)\n",
    "    print('Building-level KPIs:')\n",
    "    plt.show()\n",
    "    _ = plot_building_load_profiles(envs)\n",
    "    print('Building-level load profiles:')\n",
    "    plt.show()\n",
    "    _ = plot_battery_soc_profiles(envs)\n",
    "    print('Battery SoC profiles:')\n",
    "    plt.show()\n",
    "    _ = plot_district_kpis(envs)\n",
    "    print('District-level KPIs:')\n",
    "    plt.show()\n",
    "    print('District-level load profiles:')\n",
    "    _ = plot_district_load_profiles(envs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWXsiZ5freTG"
   },
   "source": [
    "# Custom Rule-Based Controller\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unHiw8HH1bzD"
   },
   "outputs": [],
   "source": [
    "rbc_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-qNSqYSuiY-"
   },
   "outputs": [],
   "source": [
    "class CustomRBC(HourRBC):\n",
    "   def __init__(\n",
    "       self, env: CityLearnEnv, action_map: Mapping[int, float] = None,\n",
    "       loader: IntProgress = None\n",
    "    ):\n",
    "      r\"\"\"Initialize CustomRBC.\n",
    "\n",
    "      Parameters\n",
    "      ----------\n",
    "      env: Mapping[str, CityLearnEnv]\n",
    "         CityLearn environment instance.\n",
    "      action_map: Mapping[int, float]\n",
    "         Mapping of hour to control action.\n",
    "      loader: IntProgress\n",
    "         Progress bar.\n",
    "      \"\"\"\n",
    "\n",
    "      super().__init__(env=env, action_map=action_map)\n",
    "      self.loader = loader\n",
    "\n",
    "   def next_time_step(self):\n",
    "      r\"\"\"Advance to next `time_step`.\"\"\"\n",
    "\n",
    "      super().next_time_step()\n",
    "\n",
    "      if self.loader is not None:\n",
    "         self.loader.value += 1\n",
    "      else:\n",
    "         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cr_3VaD1wbe"
   },
   "outputs": [],
   "source": [
    "action_map = {i: 0.0 for i in range(1, 25)}\n",
    "rbc_model = CustomRBC(env=rbc_env, action_map=action_map)\n",
    "print('default RBC action map:', action_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST3YhSkRIDLT"
   },
   "outputs": [],
   "source": [
    "def get_loader(**kwargs):\n",
    "    \"\"\"Returns a progress bar\"\"\"\n",
    "\n",
    "    kwargs = {\n",
    "        'value': 0,\n",
    "        'min': 0,\n",
    "        'max': 10,\n",
    "        'description': 'Simulating:',\n",
    "        'bar_style': '',\n",
    "        'style': {'bar_color': 'maroon'},\n",
    "        'orientation': 'horizontal',\n",
    "        **kwargs\n",
    "    }\n",
    "    return IntProgress(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdmxDMHJuiY_"
   },
   "outputs": [],
   "source": [
    "action_step = 0.05\n",
    "hour_step = 2\n",
    "hours = list(range(1, 25, hour_step))\n",
    "default_loader_description = 'Waiting'\n",
    "questions = \"\"\"\n",
    "<h1>Custom RBC Tuner</h1>\n",
    "<p>Use this interactive widget to tune your custom RBC!\n",
    "Reference the building load profiles above and the questions below when\n",
    "deciding on how to charge/discharge your rule-based controlled batteries.</p>\n",
    "\n",
    "<h3>Some considerations when tuning your custom RBC:</h3>\n",
    "<ul>\n",
    "    <li>What happens when actions for all hours are set to 0?</li>\n",
    "    <li>How can we set the RBC so that it takes advantage\n",
    "    of solar generation?</li>\n",
    "    <li>Can you spot the duck curve?</li>\n",
    "    <li>What settings work best for a specific building?</li>\n",
    "    <li>What settings work best for the entire district?</li>\n",
    "    <li>Can you tune the RBC to target improvements in any one of\n",
    "    the evaluation KPIs?</li>\n",
    "    <li>What challenges can you identify from this RBC tuning process?</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Interact with the controls to tune your RBC:</h3>\n",
    "\n",
    "<p>Use the sliders to set the hourly charge and discharge rate\n",
    "of the batteries. Positive values indicate charging\n",
    "and negative values indicate discharging the batteries</p>\n",
    "\"\"\"\n",
    "html_ui = HTML(value=questions, placeholder='Questions')\n",
    "sliders = [FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-1.0,\n",
    "    max=1.0,\n",
    "    step=action_step,\n",
    "    description=f'Hr: {h}-{h + hour_step - 1}',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='vertical',\n",
    "    readout=True,\n",
    "    readout_format='.2f',\n",
    ") for h in hours]\n",
    "reset_button = Button(\n",
    "    description='Reset', disabled=False, button_style='info',\n",
    "    tooltip='Set all hour actions to 0.0', icon=''\n",
    ")\n",
    "random_button = Button(\n",
    "    description='Random', disabled=False, button_style='warning',\n",
    "    tooltip='Select random hour actions', icon=''\n",
    ")\n",
    "simulate_button = Button(\n",
    "    description='Simulate', disabled=False, button_style='success',\n",
    "    tooltip='Run simulation', icon='check'\n",
    ")\n",
    "sliders_ui = HBox(sliders)\n",
    "buttons_ui = HBox([reset_button, random_button, simulate_button])\n",
    "\n",
    "# run simulation so that the environment has results\n",
    "# even if user does not interact with widgets\n",
    "sac_episodes = 1\n",
    "rbc_model.learn(episodes=sac_episodes)\n",
    "\n",
    "loader = get_loader(description=default_loader_description)\n",
    "\n",
    "def plot_building_guide(env):\n",
    "    \"\"\"Plots building load and generation profiles.\"\"\"\n",
    "\n",
    "    column_count_limit = 4\n",
    "    building_count = len(env.buildings)\n",
    "    row_count = math.ceil(building_count/column_count_limit)\n",
    "    column_count = min(column_count_limit, building_count)\n",
    "    figsize = (4.0*column_count, 1.5*row_count)\n",
    "    fig, _ = plt.subplots(row_count, column_count, figsize=figsize)\n",
    "\n",
    "    for i, (ax, b) in enumerate(zip(fig.axes, env.buildings)):\n",
    "        y1 = b.energy_simulation.non_shiftable_load\n",
    "        y2 = b.pv.get_generation(b.energy_simulation.solar_generation)\n",
    "        x = range(len(y1))\n",
    "        ax.plot(x, y1, label='Load')\n",
    "        ax.plot(x, y2, label='Generation')\n",
    "        ax.set_title(b.name)\n",
    "        ax.set_xlabel('Time step')\n",
    "        ax.set_ylabel('kWh')\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "\n",
    "        if i == building_count - 1:\n",
    "            ax.legend(\n",
    "                loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0\n",
    "            )\n",
    "        else:\n",
    "            ax.legend().set_visible(False)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def on_reset_button_clicked(b):\n",
    "    \"\"\"Zeros sliders and loader values.\"\"\"\n",
    "\n",
    "    loader.value = 0\n",
    "    loader.description = default_loader_description\n",
    "\n",
    "    for s in sliders:\n",
    "        s.value = 0.0\n",
    "\n",
    "def on_random_button_clicked(b):\n",
    "    \"\"\"Zeros loader value and sets sliders to random values.\"\"\"\n",
    "\n",
    "    loader.value = 0\n",
    "    loader.description = default_loader_description\n",
    "    options = np.arange(-1.0, 1.0, action_step)\n",
    "\n",
    "    for s in sliders:\n",
    "        s.value = round(random.choice(options), 2)\n",
    "\n",
    "def on_simulate_button_clicked(b):\n",
    "    \"\"\"Runs RBC simulation using selected action map.\"\"\"\n",
    "\n",
    "    loader.description = 'Simulating'\n",
    "    loader.value = 0\n",
    "    clear_output(wait=False)\n",
    "\n",
    "    # plot building profiles\n",
    "    _ = plot_building_guide(rbc_env)\n",
    "    plt.show()\n",
    "\n",
    "    display(html_ui, sliders_ui, buttons_ui, loader)\n",
    "    reset_button.disabled = True\n",
    "    random_button.disabled = True\n",
    "    simulate_button.disabled = True\n",
    "\n",
    "    for s in sliders:\n",
    "        s.disabled = True\n",
    "\n",
    "    action_map = {}\n",
    "\n",
    "    for h, s in zip(hours, sliders):\n",
    "        for i in range(hour_step):\n",
    "            action_map[h + i] = s.value\n",
    "\n",
    "    loader.max = rbc_env.time_steps*sac_episodes\n",
    "    rbc_model.action_map = action_map\n",
    "    rbc_model.learn(episodes=sac_episodes)\n",
    "\n",
    "    loader.description = 'Finished'\n",
    "    plot_simulation_summary({'RBC': rbc_env})\n",
    "\n",
    "    reset_button.disabled = False\n",
    "    random_button.disabled = False\n",
    "    simulate_button.disabled = False\n",
    "\n",
    "    for s in sliders:\n",
    "        s.disabled = False\n",
    "\n",
    "reset_button.on_click(on_reset_button_clicked)\n",
    "random_button.on_click(on_random_button_clicked)\n",
    "simulate_button.on_click(on_simulate_button_clicked)\n",
    "\n",
    "# plot building profiles\n",
    "_ = plot_building_guide(rbc_env)\n",
    "plt.show()\n",
    "\n",
    "# preview of building load profile\n",
    "display(html_ui, sliders_ui, buttons_ui, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KHS2--GuiY_"
   },
   "source": [
    "# Tabular Q-Learning Algorithm as an Adaptive Controller\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_j2GAa7YuiZA"
   },
   "outputs": [],
   "source": [
    "tql_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6HotiSW4Pe8"
   },
   "outputs": [],
   "source": [
    "# define active observations and actions and their bin sizes\n",
    "observation_bins = {'hour': 24}\n",
    "action_bins = {'electrical_storage': 12}\n",
    "\n",
    "# initialize list of bin sizes where each building\n",
    "# has a dictionary in the list definining its bin sizes\n",
    "observation_bin_sizes = []\n",
    "action_bin_sizes = []\n",
    "\n",
    "for b in tql_env.buildings:\n",
    "    # add a bin size definition for the buildings\n",
    "    observation_bin_sizes.append(observation_bins)\n",
    "    action_bin_sizes.append(action_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rkt9jnNuiZE"
   },
   "outputs": [],
   "source": [
    "tql_env = TabularQLearningWrapper(\n",
    "    tql_env.unwrapped,\n",
    "    observation_bin_sizes=observation_bin_sizes,\n",
    "    action_bin_sizes=action_bin_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5s9klu5nuiZF"
   },
   "outputs": [],
   "source": [
    "class CustomTabularQLearning(TabularQLearning):\n",
    "    def __init__(\n",
    "        self, env: CityLearnEnv, loader: IntProgress,\n",
    "        random_seed: int = None, **kwargs\n",
    "    ):\n",
    "        r\"\"\"Initialize CustomRBC.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: Mapping[str, CityLearnEnv]\n",
    "            CityLearn environment instance.\n",
    "        loader: IntProgress\n",
    "            Progress bar.\n",
    "        random_seed: int\n",
    "            Random number generator reprocucibility seed for\n",
    "            eqsilon-greedy action selection.\n",
    "        kwargs: dict\n",
    "            Parent class hyperparameters\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env=env, random_seed=random_seed, **kwargs)\n",
    "        self.loader = loader\n",
    "        self.reward_history = []\n",
    "\n",
    "    def next_time_step(self):\n",
    "        if self.env.time_step == 0:\n",
    "            self.reward_history.append(0)\n",
    "\n",
    "        else:\n",
    "            self.reward_history[-1] += sum(self.env.rewards[-1])\n",
    "\n",
    "        self.loader.value += 1\n",
    "        super().next_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uS7RzQUyuiZF"
   },
   "outputs": [],
   "source": [
    "# ----------------- CALCULATE NUMBER OF TRAINING EPISODES -----------------\n",
    "i = 3\n",
    "m = tql_env.observation_space[0].n\n",
    "n = tql_env.action_space[0].n\n",
    "t = tql_env.time_steps - 1\n",
    "tql_episodes = m*n*i/t\n",
    "tql_episodes = int(tql_episodes)\n",
    "print('Q-Table dimension:', (m, n))\n",
    "print('Number of episodes to train:', tql_episodes)\n",
    "\n",
    "# ------------------------------- SET LOADER ------------------------------\n",
    "loader = get_loader(max=tql_episodes*t)\n",
    "display(loader)\n",
    "\n",
    "# ----------------------- SET MODEL HYPERPARAMETERS -----------------------\n",
    "tql_kwargs = {\n",
    "    'epsilon': 1.0,\n",
    "    'minimum_epsilon': 0.01,\n",
    "    'epsilon_decay': 0.0001,\n",
    "    'learning_rate': 0.005,\n",
    "    'discount_factor': 0.99,\n",
    "}\n",
    "\n",
    "# ----------------------- INITIALIZE AND TRAIN MODEL ----------------------\n",
    "tql_model = CustomTabularQLearning(\n",
    "    env=tql_env,\n",
    "    loader=loader,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    **tql_kwargs\n",
    ")\n",
    "_ = tql_model.learn(episodes=tql_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import gym\n",
    "print(gymnasium.__version__)\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uG900pOuiZH"
   },
   "outputs": [],
   "source": [
    "observations = tql_env.reset()\n",
    "\n",
    "while not tql_env.done:\n",
    "    actions = tql_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = tql_env.step(actions)\n",
    "\n",
    "# plot summary and compare with other control results\n",
    "plot_simulation_summary({'RBC': rbc_env, 'TQL': tql_env})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3OX9GNVnEDt"
   },
   "outputs": [],
   "source": [
    "def plot_table(\n",
    "    ax: plt.Axes, table: np.ndarray, title: str, cmap: str,\n",
    "    colorbar_label: str, xlabel: str, ylabel: str\n",
    ") -> plt.Axes:\n",
    "    \"\"\"Plot 2-dimensional table on a heat map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: plt.Axes\n",
    "        Figure axes\n",
    "    table: np.ndarray\n",
    "        Table array\n",
    "    title: str\n",
    "        axes title\n",
    "    cmap: str\n",
    "        Colormap\n",
    "    colorbar_label: str\n",
    "        Colorbar name\n",
    "    xlabel: str\n",
    "        Heat map x-axis label\n",
    "    ylabel: str\n",
    "        Heat map y-axis label\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: plt.Axes\n",
    "        Plotted axes\n",
    "    \"\"\"\n",
    "\n",
    "    x = list(range(table.shape[0]))\n",
    "    y = list(range(table.shape[1]))\n",
    "    z = table.T\n",
    "    pcm = ax.pcolormesh(\n",
    "        x, y, z, shading='nearest', cmap=cmap,\n",
    "        edgecolors='black', linewidth=0.0\n",
    "    )\n",
    "    _ = fig.colorbar(\n",
    "        pcm, ax=ax, orientation='horizontal',\n",
    "        label=colorbar_label, fraction=0.025, pad=0.08\n",
    "    )\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hdSMspcn3jFG"
   },
   "outputs": [],
   "source": [
    "cmap = 'coolwarm'\n",
    "figsize = (12, 8)\n",
    "fig, axs = plt.subplots(1, 3, figsize=figsize, sharey=True)\n",
    "axs[0] = plot_table(\n",
    "    axs[0], tql_model.q[0], 'Q-Table',\n",
    "    cmap, 'Q-Value', 'State (Hour)', 'Action Index'\n",
    ")\n",
    "axs[1] = plot_table(\n",
    "    axs[1], tql_model.q_exploration[0], 'Q-Table Exploration',\n",
    "    cmap, 'Count', 'State (Hour)', None\n",
    ")\n",
    "axs[2] = plot_table(\n",
    "    axs[2], tql_model.q_exploitation[0], 'Q-Table Exploitation',\n",
    "    cmap, 'Count', 'State (Hour)', None\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2Alb3JD3o6b"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Current Tabular Q-Learning epsilon after {tql_episodes}'\\\n",
    "        f' episodes and {tql_model.time_step} time steps:', tql_model.epsilon\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDjatcra3vPG"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 2))\n",
    "y = np.array([max(\n",
    "    tql_model.minimum_epsilon,\n",
    "    tql_model.epsilon_init*np.exp(-tql_model.epsilon_decay*e)\n",
    ") for e in range(100_000)])\n",
    "ref_x = len(y) - len(y[y <= 0.5]) - 1\n",
    "ref_y = y[ref_x]\n",
    "ax.plot(y)\n",
    "ax.axvline(ref_x, color='red', linestyle=':')\n",
    "ax.axhline(ref_y, color='red', linestyle=':')\n",
    "ax.axvline(tql_episodes, color='green', linestyle=':')\n",
    "ax.set_xlabel('Episode')\n",
    "text = f'{ref_x} training episodes needed to get\\nat least 50%'\\\n",
    "    ' exploitation probability.'\n",
    "ax.text(ref_x + 1000, ref_y + 0.05, text, color='red')\n",
    "ax.text(\n",
    "    tql_episodes + 1000,\n",
    "    ref_y - 0.1,\n",
    "    f'Current training episodes = {tql_episodes}',\n",
    "    va='bottom', color='green'\n",
    ")\n",
    "ax.set_ylabel(r'$\\epsilon$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q74Y_l8a369T"
   },
   "source": [
    "# Soft-Actor Critic Reinforcement Learning Controller (SAC)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9A8-38t390y"
   },
   "outputs": [],
   "source": [
    "sac_env = CityLearnEnv(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBH83tFY4DhV"
   },
   "outputs": [],
   "source": [
    "sac_env = NormalizedObservationWrapper(sac_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Yq5edYr4JXo"
   },
   "outputs": [],
   "source": [
    "sac_env = StableBaselines3Wrapper(sac_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLkcTNQ34NLy"
   },
   "outputs": [],
   "source": [
    "sac_model = SAC(policy='MlpPolicy', env=sac_env, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtnL5S394TJB"
   },
   "outputs": [],
   "source": [
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env: CityLearnEnv, loader: IntProgress):\n",
    "        r\"\"\"Initialize CustomCallback.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env: Mapping[str, CityLearnEnv]\n",
    "            CityLearn environment instance.\n",
    "        loader: IntProgress\n",
    "            Progress bar.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(verbose=0)\n",
    "        self.loader = loader\n",
    "        self.env = env\n",
    "        self.reward_history = [0]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        r\"\"\"Called each time the env step function is called.\"\"\"\n",
    "\n",
    "        if self.env.time_step == 0:\n",
    "            self.reward_history.append(0)\n",
    "\n",
    "        else:\n",
    "            self.reward_history[-1] += sum(self.env.rewards[-1])\n",
    "\n",
    "        self.loader.value += 1\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hpytx_Rz4onF"
   },
   "outputs": [],
   "source": [
    "# ----------------- CALCULATE NUMBER OF TRAINING EPISODES -----------------\n",
    "fraction = 0.25\n",
    "sac_episodes = int(tql_episodes*fraction)\n",
    "print('Fraction of Tabular Q-Learning episodes used:', fraction)\n",
    "print('Number of episodes to train:', sac_episodes)\n",
    "sac_episode_timesteps = sac_env.time_steps - 1\n",
    "sac_total_timesteps = sac_episodes*sac_episode_timesteps\n",
    "\n",
    "# ------------------------------- SET LOADER ------------------------------\n",
    "sac_loader = get_loader(max=sac_total_timesteps)\n",
    "display(sac_loader)\n",
    "\n",
    "# ------------------------------- TRAIN MODEL -----------------------------\n",
    "sac_callback = CustomCallback(env=sac_env, loader=sac_loader)\n",
    "sac_model = sac_model.learn(\n",
    "    total_timesteps=sac_total_timesteps,\n",
    "    callback=sac_callback\n",
    ")\n",
    "\n",
    "sac_rewards = sac_callback.reward_history[1:]  # removes first 0 inserted in callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SxBBofg5pgL"
   },
   "outputs": [],
   "source": [
    "observations = sac_env.reset()\n",
    "sac_actions_list = []\n",
    "\n",
    "while not sac_env.done:\n",
    "    actions, _ = sac_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = sac_env.step(actions)\n",
    "    sac_actions_list.append(actions)\n",
    "\n",
    "# plot summary and compare with other control results\n",
    "plot_simulation_summary({'RBC': rbc_env, 'TQL': tql_env, 'SAC-1': sac_env})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLIKfCXO5xiD"
   },
   "outputs": [],
   "source": [
    "def plot_actions(actions_list: List[List[float]], title: str) -> plt.Figure:\n",
    "    \"\"\"Plots action time series for different buildings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actions_list: List[List[float]]\n",
    "        List of actions where each element with index, i,\n",
    "        in list is a list of the actions for different buildings\n",
    "        taken at time step i.\n",
    "    title: str\n",
    "        Plot axes title\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig: plt.Figure\n",
    "        Figure with plotted axes\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 1))\n",
    "    columns = [b.name for b in sac_env.buildings]\n",
    "    plot_data = pd.DataFrame(actions_list, columns=columns)\n",
    "    x = list(range(plot_data.shape[0]))\n",
    "\n",
    "    for c in plot_data.columns:\n",
    "        y = plot_data[c].tolist()\n",
    "        ax.plot(x, y, label=c)\n",
    "\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0), framealpha=0.0)\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel(r'$\\frac{kWh}{kWh_{capacity}}$')\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(24))\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_actions(sac_actions_list, 'SAC-1 Actions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dt77XG7I6Fd6"
   },
   "source": [
    "## Custom Reward Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPK08TkI6Jsi"
   },
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Union\n",
    "from citylearn.reward_function import RewardFunction\n",
    "\n",
    "class CustomReward(RewardFunction):\n",
    "    r\"\"\"Initialize CustomReward.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env_metadata: Mapping[str, Any]\n",
    "        General static information about the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_metadata: Mapping[str, Any]):\n",
    "        super().__init__(env_metadata)\n",
    "\n",
    "    def calculate(\n",
    "        self, observations: List[Mapping[str, Union[int, float]]]\n",
    "    ) -> List[float]:\n",
    "        r\"\"\"Returns reward for the most recent action.\n",
    "\n",
    "        The reward is designed to minimize electricity cost.\n",
    "        It is calculated for each building, *i*, and summed to provide the agent\n",
    "        with a reward that is representative of all *n* buildings.\n",
    "        It encourages net-zero energy use by penalizing grid load satisfaction\n",
    "        when there is energy in the battery, as well as penalizing\n",
    "        net export when the battery is not fully charged (through the penalty\n",
    "        term). There is neither penalty nor reward when the battery\n",
    "        is fully charged during net export to the grid. Whereas, when the\n",
    "        battery is charged to capacity and there is net import from the\n",
    "        grid, the penalty is maximized.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations: List[Mapping[str, Union[int, float]]]\n",
    "            List of all building observations at the current \n",
    "            :py:attr:`citylearn.citylearn.CityLearnEnv.time_step`\n",
    "            (obtained by calling :py:meth:`citylearn.building.Building.observations`).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        reward: List[float]\n",
    "            Reward for the transition to the current timestep.\n",
    "        \"\"\"\n",
    "        reward_list = []\n",
    "\n",
    "        for o, m in zip(observations, self.env_metadata['buildings']):\n",
    "            cost = o['net_electricity_consumption'] * o['electricity_pricing']\n",
    "            battery_capacity = m['electrical_storage']['capacity']\n",
    "            battery_soc = o.get('electrical_storage_soc', 0.0)\n",
    "            penalty = -(1.0 + np.sign(cost) * battery_soc)\n",
    "            reward = penalty * abs(cost)\n",
    "            reward_list.append(reward)\n",
    "\n",
    "        reward = [sum(reward_list)]\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38i9ZAnp6Ns7"
   },
   "outputs": [],
   "source": [
    "# ----------------- INITIALIZE ENVIRONMENT -----------------\n",
    "sacr_env = CityLearnEnv(schema)\n",
    "\n",
    "# -------------------- SET CUSTOM REWARD -------------------\n",
    "sacr_env.reward_function = CustomReward(sacr_env.get_metadata())\n",
    "\n",
    "# -------------------- WRAP ENVIRONMENT --------------------\n",
    "sacr_env = NormalizedObservationWrapper(sacr_env)\n",
    "sacr_env = StableBaselines3Wrapper(sacr_env)\n",
    "\n",
    "# -------------------- INITIALIZE AGENT --------------------\n",
    "sacr_model = SAC(policy='MlpPolicy', env=sacr_env, seed=RANDOM_SEED)\n",
    "\n",
    "\n",
    "# ----------------------- SET LOADER -----------------------\n",
    "print('Number of episodes to train:', sac_episodes)\n",
    "sac_modr_loader = get_loader(max=sac_total_timesteps)\n",
    "display(sac_modr_loader)\n",
    "\n",
    "# ----------------------- TRAIN AGENT ----------------------\n",
    "sacr_callback = CustomCallback(env=sacr_env, loader=sac_modr_loader)\n",
    "sacr_model = sacr_model.learn(\n",
    "    total_timesteps=sac_total_timesteps,\n",
    "    callback=sacr_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9uB1Fr56TmN"
   },
   "outputs": [],
   "source": [
    "observations = sacr_env.reset()\n",
    "sacr_actions_list = []\n",
    "\n",
    "while not sacr_env.done:\n",
    "    actions, _ = sacr_model.predict(observations, deterministic=True)\n",
    "    observations, _, _, _ = sacr_env.step(actions)\n",
    "    sacr_actions_list.append(actions)\n",
    "\n",
    "plot_simulation_summary(\n",
    "    {'RBC': rbc_env, 'TQL': tql_env, 'SAC-1': sac_env, 'SAC-2': sacr_env}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s2C5gOf6aSO"
   },
   "outputs": [],
   "source": [
    "fig = plot_actions(sacr_actions_list, 'SAC Actions using Custom Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D-Qlfv-6kNI"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(ax: plt.Axes, rewards: List[float], title: str) -> plt.Axes:\n",
    "    \"\"\"Plots rewards over training episodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards: List[float]\n",
    "        List of reward sum per episode.\n",
    "    title: str\n",
    "        Plot axes title\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: plt.Axes\n",
    "        Plotted axes\n",
    "    \"\"\"\n",
    "\n",
    "    ax.plot(rewards)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Reward')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZOaEMwQ6lqa"
   },
   "outputs": [],
   "source": [
    "rewards = {\n",
    "    'Tabular Q-Learning': tql_model.reward_history[:tql_episodes],\n",
    "    'SAC-1': sac_callback.reward_history[:sac_episodes],\n",
    "    'SAC-2': sacr_callback.reward_history[:sac_episodes]\n",
    "}\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 2))\n",
    "\n",
    "for ax, (k, v) in zip(fig.axes, rewards.items()):\n",
    "    ax = plot_rewards(ax, v, k)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§  Soft Actor-Critic (SAC) Optimization for CityLearn Environment\n",
    "\n",
    "This section implements the Soft Actor-Critic (SAC) algorithm, a model-free off-policy reinforcement learning method that combines the benefits of both value-based and policy-based approaches. SAC is particularly effective for continuous control tasks like building energy management in the CityLearn environment.\n",
    "\n",
    "## ðŸ”§ Implementation Overview\n",
    "\n",
    "- **Algorithm**: Soft Actor-Critic (SAC)\n",
    "- **Environment**: CityLearn with custom reward function\n",
    "- **Training Episodes**: 1000 (optimized for convergence)\n",
    "- **Seeds**: 5 different random seeds for robust evaluation\n",
    "- **Wrappers**:\n",
    "  - `NormalizedObservationWrapper`: Normalizes observations for stable training\n",
    "  - `StableBaselines3Wrapper`: Ensures compatibility with SB3\n",
    "\n",
    "## ðŸŽ¯ Key Hyperparameters\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Policy | MlpPolicy | Multi-Layer Perceptron policy |\n",
    "| Learning Rate | 3e-4 | Optimizer step size |\n",
    "| Batch Size | 512 | Size of minibatches for training |\n",
    "| Gamma (Î³) | 0.99 | Discount factor for future rewards |\n",
    "| Tau (Ï„) | 0.005 | Soft update coefficient for target networks |\n",
    "| Entropy Coefficient | Auto-tuned | Balances exploration/exploitation |\n",
    "\n",
    "## ðŸ“Š Training Process\n",
    "\n",
    "1. **Environment Setup**:\n",
    "   - Initialize CityLearn environment with specified schema\n",
    "   - Apply observation normalization\n",
    "   - Set up SB3 compatibility\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   - Initialize SAC with specified hyperparameters\n",
    "   - Set random seeds for reproducibility\n",
    "   - Configure custom callback for reward tracking\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - Run for specified number of episodes\n",
    "   - Save model checkpoints and reward history\n",
    "   - Track progress with visual feedback\n",
    "\n",
    "4. **Output**:\n",
    "   - Save trained models\n",
    "   - Log reward trajectories\n",
    "   - Generate learning curves\n",
    "\n",
    "## ðŸ’¡ Key Features\n",
    "\n",
    "- **Off-policy Learning**: Efficient sample reuse through experience replay\n",
    "- **Entropy Regularization**: Automatically balances exploration and exploitation\n",
    "- **Stable Training**: Uses target networks and soft updates\n",
    "- **Reproducibility**: Multiple seeds for reliable evaluation\n",
    "\n",
    "The code includes comprehensive progress tracking and automatically saves results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "fraction = 0.25\n",
    "# sac_episodes = int(tql_episodes * fraction)\n",
    "sac_episodes = 10  # number of episodes for clear convergence to SAC stability\n",
    "\n",
    "print(f'Number of episodes to train SAC: {sac_episodes}')\n",
    "\n",
    "# To store all rewards\n",
    "all_sac_rewards = []\n",
    "\n",
    "# Training for each seed\n",
    "for seed in seeds:\n",
    "    print(f'\\nðŸš€ Starting SAC training with seed {seed}')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    sac_env = CityLearnEnv(schema)\n",
    "    sac_env.reward_function = CustomReward(sac_env.get_metadata())\n",
    "    sac_env = NormalizedObservationWrapper(sac_env)\n",
    "    sac_env = StableBaselines3Wrapper(sac_env)\n",
    "\n",
    "    sac_episode_timesteps = sac_env.time_steps - 1\n",
    "    sac_total_timesteps = sac_episodes * sac_episode_timesteps\n",
    "\n",
    "    sac_loader = get_loader(max=sac_total_timesteps)\n",
    "    display(sac_loader)\n",
    "\n",
    "    sac_callback = CustomCallback(env=sac_env, loader=sac_loader)\n",
    "\n",
    "    sac_model = SAC(\n",
    "        policy='MlpPolicy',\n",
    "        env=sac_env,\n",
    "        seed=seed,\n",
    "        learning_rate=3e-4,\n",
    "        batch_size=512,\n",
    "        gamma=0.99,\n",
    "        tau=0.005,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    sac_model.learn(\n",
    "        total_timesteps=sac_total_timesteps,\n",
    "        callback=sac_callback\n",
    "    )\n",
    "\n",
    "    rewards = pd.Series(sac_callback.reward_history[1:])  # remove initial reward = 0\n",
    "    all_sac_rewards.append(rewards)\n",
    "\n",
    "    rewards.to_csv(f'results/sac_rewards_seed{seed}.csv', index=False)\n",
    "    print(f'âœ… Seed {seed} complete â€” results saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate mean and standard deviation of rewards per episode\n",
    "df_sac_all = pd.concat(all_sac_rewards, axis=1)\n",
    "df_sac_all.columns = [f'Seed {s}' for s in seeds]\n",
    "\n",
    "reward_mean = df_sac_all.mean(axis=1)\n",
    "reward_std = df_sac_all.std(axis=1)\n",
    "reward_smoothed = reward_mean.rolling(window=10).mean()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(reward_smoothed, color='blue', linewidth=2, label='Mean')\n",
    "\n",
    "# Â±1 standard deviation bands\n",
    "ax.fill_between(\n",
    "    reward_mean.index,\n",
    "    reward_mean - reward_std,\n",
    "    reward_mean + reward_std,\n",
    "    color='blue',\n",
    "    alpha=0.1,\n",
    "    label='Â±1 std'\n",
    ")\n",
    "\n",
    "# Labels and layout\n",
    "ax.set_title('SAC (multi-seed)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Proximal Policy Optimization (PPO) for CityLearn Environment\n",
    "\n",
    "This code implements the Proximal Policy Optimization (PPO) algorithm, a policy gradient method that's particularly effective for continuous control tasks in the CityLearn environment.\n",
    "\n",
    "## ðŸ”§ Implementation Overview\n",
    "\n",
    "- **Algorithm**: Proximal Policy Optimization (PPO)\n",
    "- **Environment**: CityLearn with custom reward function\n",
    "- **Training Episodes**: 1000\n",
    "- **Seeds**: 5 different random seeds for robust evaluation\n",
    "- **Wrappers**:\n",
    "  - `NormalizedObservationWrapper`: Normalizes observations for stable training\n",
    "  - `StableBaselines3Wrapper`: Ensures compatibility with SB3\n",
    "\n",
    "## ðŸŽ¯ Key Hyperparameters\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Policy | MlpPolicy | Multi-Layer Perceptron policy |\n",
    "| Learning Rate | 3e-4 | Optimizer step size |\n",
    "| Batch Size | 64 | Size of minibatches for training |\n",
    "| N Steps | 1024 | Number of steps per update |\n",
    "| N Epochs | 10 | Number of epochs for optimization |\n",
    "| Gamma (Î³) | 0.99 | Discount factor for future rewards |\n",
    "| Clip Range | 0.2 | Clipping parameter for policy updates |\n",
    "| Entropy Coefficient | 0.01 | Encourages exploration |\n",
    "| Target KL | 0.05 | Maximum KL divergence between updates |\n",
    "\n",
    "## ðŸ“Š Training Process\n",
    "\n",
    "1. **Environment Setup**:\n",
    "   - Initialize CityLearn environment with specified schema\n",
    "   - Apply observation normalization\n",
    "   - Set up SB3 compatibility\n",
    "\n",
    "2. **Model Configuration**:\n",
    "   - Initialize PPO with specified hyperparameters\n",
    "   - Set random seeds for reproducibility\n",
    "   - Configure custom callback for reward tracking\n",
    "\n",
    "3. **Training Loop**:\n",
    "   - Run for specified number of episodes\n",
    "   - Save model checkpoints and reward history\n",
    "   - Track progress with visual feedback\n",
    "\n",
    "4. **Output**:\n",
    "   - Save trained models\n",
    "   - Log reward trajectories\n",
    "   - Generate learning curves\n",
    "\n",
    "## ðŸ’¡ Key Features\n",
    "\n",
    "- **Trust Region Updates**: Uses clipping to ensure stable policy updates\n",
    "- **Multiple Epochs**: Reuses samples for multiple gradient steps\n",
    "- **Entropy Regularization**: Balances exploration and exploitation\n",
    "- **Reproducibility**: Multiple seeds for reliable evaluation\n",
    "\n",
    "The code includes comprehensive progress tracking and automatically saves results for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Environment and Wrapper Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO            #Imports PPO algorithm from SB3\n",
    "\n",
    "# ppo_env = CityLearnEnv(schema)               #Creates CityLearn env with same schema\n",
    "# ppo_env = NormalizedObservationWrapper(ppo_env)        #Normalization of observations, as is done with SAC\n",
    "# ppo_env = StableBaselines3Wrapper(ppo_env)             #Wrapper SB3 for compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Interpretation of PPO Training Parameters\n",
    "\n",
    "| Section        | Key                | Meaning                                                                                   |\n",
    "|----------------|-----------------------|-----------------------------------------------------------------------------------------------|\n",
    "| **rollout/**   | `ep_len_mean`         | Average episode length (in steps). Full episodes: the entire simulated year has been used â†’ âœ… |\n",
    "|                | `ep_rew_mean`         | Average reward per episode (expected value: should improve, ideally trend toward zero)       |\n",
    "| **time/**      | `fps`                 | Frames per second (simulation speed)                                                |\n",
    "|                | `iterations`          | Total number of updates (one every n_steps)                                      |\n",
    "|                | `time_elapsed`        | Total training time (in seconds) â±ï¸                                                    |\n",
    "|                | `total_timesteps`     | Total number of simulated steps (episodes Ã— steps per episode)                               |\n",
    "| **train/**     | `approx_kl`           | KL divergence between old and new policy (lower is better â†’ more stable updates)  |\n",
    "|                | `clip_fraction`       | Percentage of clipped updates (outside threshold)                                                 |\n",
    "|                | `clip_range`          | Maximum allowed range for policy updates                                             |\n",
    "|                | `entropy_loss`        | Entropy loss (how â€œrandomâ€ the policy is; large negative values = more deterministic) ðŸŽ² |\n",
    "|                | `explained_variance`  | Variance explained by the value function vs. the target (higher = more accurate value network)   |\n",
    "|                | `learning_rate`       | Learning rate. 0.0001 is considered a **very stable** value ðŸ“‰                                |\n",
    "|                | `loss`                | Total loss (sum of policy loss and value loss)                                          |\n",
    "|                | `n_updates`           | Total number of policy/value updates                                                  |\n",
    "|                | `policy_gradient_loss`| How much the policy has been updated (gradient of the loss)                                   |\n",
    "|                | `std`                 | Standard deviation of sampled actions (low = less exploration)                      |\n",
    "|                | `value_loss`          | Value function error (lower is better)                                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "ppo_episodes = 10\n",
    "print(f'Number of episodes to train PPO: {ppo_episodes}')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "all_ppo_rewards = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f'\\nðŸš€ Starting PPO training with seed {seed}')\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Initialize environment and wrapper\n",
    "    env = CityLearnEnv(schema)\n",
    "    env.reward_function = CustomReward(env.get_metadata())\n",
    "    env = NormalizedObservationWrapper(env)\n",
    "    env = StableBaselines3Wrapper(env)\n",
    "\n",
    "    # Calculate timesteps\n",
    "    episode_timesteps = env.time_steps - 1\n",
    "    total_timesteps = ppo_episodes * episode_timesteps\n",
    "\n",
    "    loader = get_loader(max=total_timesteps)\n",
    "    display(loader)\n",
    "    callback = CustomCallback(env=env, loader=loader)\n",
    "\n",
    "    model = PPO(\n",
    "        policy='MlpPolicy',\n",
    "        env=env,\n",
    "        seed=seed,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01,\n",
    "        target_kl=0.05,\n",
    "        normalize_advantage=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=callback\n",
    "    )\n",
    "\n",
    "    rewards = pd.Series(callback.reward_history[:ppo_episodes])\n",
    "    all_ppo_rewards.append(rewards)\n",
    "\n",
    "    rewards.to_csv(f'results/ppo_rewards_seed{seed}.csv', index=False)\n",
    "    model.save(f'models/ppo_model_seed{seed}')\n",
    "\n",
    "    print(f'âœ… Seed {seed} complete â€” results saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Combine reward history from all seeds into a DataFrame\n",
    "df_ppo_all = pd.concat(all_ppo_rewards, axis=1)\n",
    "df_ppo_all.columns = [f'Seed {s}' for s in seeds]\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "ppo_mean = df_ppo_all.mean(axis=1)\n",
    "ppo_std = df_ppo_all.std(axis=1)\n",
    "ppo_smoothed = ppo_mean.rolling(window=25).mean()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# (Optional) Plot raw rewards for each seed\n",
    "# for i, seed in enumerate(seeds):\n",
    "#     ax.plot(df_ppo_all.iloc[:, i], alpha=0.2, label=f'Seed {seed}')\n",
    "\n",
    "ax.plot(ppo_smoothed, color='green', linewidth=2, label='Mean')\n",
    "\n",
    "# Â±1 standard deviation bands\n",
    "ax.fill_between(\n",
    "    ppo_mean.index,\n",
    "    ppo_mean - ppo_std,\n",
    "    ppo_mean + ppo_std,\n",
    "    color='green',\n",
    "    alpha=0.1,\n",
    "    label='Â±1 std'\n",
    ")\n",
    "\n",
    "# Labels and layout\n",
    "ax.set_title('PPO (multi-seed)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Twin Delayed DDPG (TD3) for CityLearn Environment\n",
    "\n",
    "This code implements the Twin Delayed DDPG (TD3) algorithm, an advanced off-policy method for continuous control in the CityLearn environment. TD3 improves upon DDPG by addressing function approximation errors through twin Q-networks and delayed policy updates.\n",
    "\n",
    "## ðŸ”§ Implementation Overview\n",
    "\n",
    "- **Algorithm**: Twin Delayed DDPG (TD3)\n",
    "- **Environment**: CityLearn with custom reward function\n",
    "- **Training Episodes**: 1000\n",
    "- **Seeds**: 5 different random seeds for robust evaluation\n",
    "- **Device**: Automatically uses GPU if available, falls back to CPU\n",
    "- **Wrappers**:\n",
    "  - `NormalizedObservationWrapper`: Normalizes observations for stable training\n",
    "  - `StableBaselines3Wrapper`: Ensures compatibility with SB3\n",
    "\n",
    "## ðŸŽ¯ Key Hyperparameters\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| Policy | MlpPolicy | Multi-Layer Perceptron policy |\n",
    "| Learning Rate | 1e-3 | Optimizer step size |\n",
    "| Batch Size | 128 | Size of minibatches for training |\n",
    "| Buffer Size | 50,000 | Size of experience replay buffer |\n",
    "| Tau (Ï„) | 0.001 | Soft update coefficient for target networks |\n",
    "| Gamma (Î³) | 0.99 | Discount factor for future rewards |\n",
    "| Policy Delay | 2 | Frequency of policy updates relative to Q-functions |\n",
    "| Target Noise | 0.2 | Stddev of noise added to target actions |\n",
    "| Noise Clip | 0.3 | Range for clipping target noise |\n",
    "\n",
    "## ðŸ“Š Training Process\n",
    "\n",
    "1. **Environment Setup**:\n",
    "   - Initialize CityLearn environment with specified schema\n",
    "   - Apply observation normalization\n",
    "   - Set up SB3 compatibility\n",
    "\n",
    "2. **Exploration Strategy**:\n",
    "   - Uses NormalActionNoise for exploration\n",
    "   - Noise parameters: mean=0, sigma=0.1\n",
    "\n",
    "3. **Model Configuration**:\n",
    "   - Twin Q-networks to prevent overestimation\n",
    "   - Delayed policy updates for stability\n",
    "   - Experience replay buffer for sample efficiency\n",
    "\n",
    "4. **Training Loop**:\n",
    "   - Runs for specified number of episodes\n",
    "   - Saves model checkpoints and reward history\n",
    "   - Tracks progress with visual feedback\n",
    "\n",
    "5. **Output**:\n",
    "   - Saves trained models\n",
    "   - Logs reward trajectories\n",
    "   - Clears GPU cache between seeds\n",
    "\n",
    "## ðŸ’¡ Key Features\n",
    "\n",
    "- **Twin Q-Networks**: Reduces overestimation bias\n",
    "- **Delayed Updates**: Improves stability by updating policy less frequently\n",
    "- **Target Policy Smoothing**: Adds noise to target actions\n",
    "- **GPU Acceleration**: Automatically utilizes available GPU resources\n",
    "- **Reproducibility**: Multiple seeds for reliable evaluation\n",
    "- **Memory Management**: Clears GPU cache between training runs\n",
    "\n",
    "The code includes comprehensive progress tracking and automatically saves results for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Training parameters\n",
    "td3_episodes = 10\n",
    "print('Number of episodes to train TD3:', td3_episodes)\n",
    "\n",
    "# Multi-seed setup\n",
    "seeds = [0, 1, 2, 3, 4]\n",
    "all_td3_rewards = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"\\n=== Training TD3 with seed {seed} ===\")\n",
    "\n",
    "    # Initialize CityLearn environment\n",
    "    td3_env = CityLearnEnv(schema)\n",
    "    td3_env.reward_function = CustomReward(td3_env.get_metadata())  # Align TD3 reward signal with SAC/PPO\n",
    "    td3_env = NormalizedObservationWrapper(td3_env)\n",
    "    td3_env = StableBaselines3Wrapper(td3_env)\n",
    "\n",
    "    # Exploration noise\n",
    "    n_actions = td3_env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(\n",
    "        mean=np.zeros(n_actions),\n",
    "        sigma=0.1 * np.ones(n_actions)\n",
    "    )\n",
    "\n",
    "    # Calculate total timesteps\n",
    "    td3_episode_timesteps = td3_env.time_steps - 1\n",
    "    td3_total_timesteps = td3_episodes * td3_episode_timesteps\n",
    "\n",
    "    # Progress bar loader\n",
    "    td3_loader = get_loader(max=td3_total_timesteps)\n",
    "    display(td3_loader)\n",
    "\n",
    "    # Callback for tracking rewards\n",
    "    td3_callback = CustomCallback(env=td3_env, loader=td3_loader)\n",
    "\n",
    "    # TD3 model\n",
    "    td3_model = TD3(\n",
    "        policy='MlpPolicy',\n",
    "        env=td3_env,\n",
    "        seed=seed,\n",
    "        learning_rate=1e-3,\n",
    "        buffer_size=50_000,\n",
    "        batch_size=128,\n",
    "        tau=0.001,\n",
    "        gamma=0.99,\n",
    "        train_freq=(1, 'step'),\n",
    "        gradient_steps=1,\n",
    "        learning_starts=1000,\n",
    "        policy_delay=2,\n",
    "        target_policy_noise=0.2,\n",
    "        target_noise_clip=0.3,\n",
    "        action_noise=action_noise,\n",
    "        verbose=0,\n",
    "        device=device,\n",
    "        policy_kwargs=dict(\n",
    "            net_arch=dict(pi=[256, 256], qf=[256, 256]),  # Network architecture\n",
    "            activation_fn=torch.nn.ReLU,\n",
    "            n_critics=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    td3_model.learn(\n",
    "        total_timesteps=td3_total_timesteps,\n",
    "        callback=td3_callback\n",
    "    )\n",
    "\n",
    "    # Save rewards\n",
    "    current_seed_rewards = pd.Series(td3_callback.reward_history)\n",
    "    all_td3_rewards.append(current_seed_rewards)\n",
    "\n",
    "    current_seed_rewards.to_csv(f'results/td3_rewards_seed{seed}.csv', index=False)\n",
    "    td3_model.save(f'models/td3_model_seed{seed}')\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'âœ… Seed {seed} complete â€” results saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with rewards from each seed\n",
    "df_all_td3 = pd.concat(all_td3_rewards, axis=1)\n",
    "df_all_td3.columns = [f'Seed {s}' for s in seeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Combine reward history from all seeds into a DataFrame\n",
    "df_td3_all = pd.concat(all_td3_rewards, axis=1)\n",
    "df_td3_all.columns = [f'Seed {s}' for s in seeds]\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "td3_mean = df_td3_all.mean(axis=1)\n",
    "td3_std = df_td3_all.std(axis=1)\n",
    "td3_smoothed = td3_mean.rolling(window=25).mean()\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# (Optional) Plot raw rewards for each seed\n",
    "# for i, seed in enumerate(seeds):\n",
    "#     ax.plot(df_td3_all.iloc[:, i], alpha=0.2, label=f'Seed {seed}')\n",
    "\n",
    "ax.plot(td3_smoothed, color='blue', linewidth=2, label='Mean')\n",
    "\n",
    "# Add Â±1 standard deviation bands\n",
    "ax.fill_between(\n",
    "    td3_mean.index,\n",
    "    td3_mean - td3_std,\n",
    "    td3_mean + td3_std,\n",
    "    color='blue',\n",
    "    alpha=0.1,\n",
    "    label='Â±1 std'\n",
    ")\n",
    "\n",
    "# Configure plot labels and layout\n",
    "ax.set_title('TD3 (multi-seed)')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Performance Comparison: SAC vs PPO vs TD3\n",
    "\n",
    "This code generates a comparative visualization of the training performance across three reinforcement learning algorithms: SAC, PPO, and TD3. The plot provides insights into the learning dynamics and stability of each algorithm.\n",
    "\n",
    "## ðŸ” What the Code Does:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Expects three DataFrames (`df_sac_all`, `df_ppo_all`, `df_td3_all`), each containing results from 5 different random seeds\n",
    "   - Each column represents the reward trajectory from a different seed\n",
    "\n",
    "2. **Statistical Analysis**:\n",
    "   - Computes mean and standard deviation across seeds for each algorithm\n",
    "   - Applies a rolling average (window=25) to smooth the learning curves\n",
    "   - Handles confidence intervals using Â±1 standard deviation\n",
    "\n",
    "3. **Visualization**:\n",
    "   - Plots smoothed mean reward curves for SAC (blue), PPO (green), and TD3 (orange)\n",
    "   - Adds semi-transparent bands showing the standard deviation across seeds\n",
    "   - Includes proper labels, title, grid, and legend for clarity\n",
    "\n",
    "## ðŸ“ˆ Key Features:\n",
    "\n",
    "- **Multi-seed Analysis**: Shows both central tendency and variance across different random initializations\n",
    "- **Smoothed Curves**: Uses rolling average to highlight overall trends\n",
    "- **Confidence Intervals**: Visualizes algorithm stability through standard deviation bands\n",
    "- **Comparison**: Enables direct performance comparison between the three algorithms\n",
    "\n",
    "The resulting plot helps evaluate:\n",
    "- Which algorithm converges faster\n",
    "- Which algorithm achieves higher rewards\n",
    "- The stability of each algorithm across different random seeds\n",
    "- The variance in performance between different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# === INPUT ===\n",
    "# Assicurati che i seguenti DataFrame esistano e contengano 5 colonne (1 per seed):\n",
    "# df_sac_all, df_ppo_all, df_td3_all\n",
    "\n",
    "# Funzione di utilitÃ  per calcolare media, std e smoothing\n",
    "def compute_stats(df, window=25):\n",
    "    mean = df.mean(axis=1)\n",
    "    std = df.std(axis=1)\n",
    "    smoothed = mean.rolling(window=window).mean()\n",
    "    return mean, std, smoothed\n",
    "\n",
    "# Calcolo statistiche\n",
    "sac_mean, sac_std, sac_smooth = compute_stats(df_sac_all)\n",
    "ppo_mean, ppo_std, ppo_smooth = compute_stats(df_ppo_all)\n",
    "td3_mean, td3_std, td3_smooth = compute_stats(df_td3_all)\n",
    "\n",
    "# === PLOT ===\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "# SAC\n",
    "ax.plot(sac_smooth, label='SAC (media)', linewidth=2, color='tab:blue')\n",
    "ax.fill_between(sac_mean.index, sac_mean - sac_std, sac_mean + sac_std,\n",
    "                alpha=0.1, color='tab:blue', label='SAC Â±1 std')\n",
    "\n",
    "# PPO\n",
    "ax.plot(ppo_smooth, label='PPO (media)', linewidth=2, color='tab:green')\n",
    "ax.fill_between(ppo_mean.index, ppo_mean - ppo_std, ppo_mean + ppo_std,\n",
    "                alpha=0.1, color='tab:green', label='PPO Â±1 std')\n",
    "\n",
    "# TD3\n",
    "ax.plot(td3_smooth, label='TD3 (media)', linewidth=2, color='tab:orange')\n",
    "ax.fill_between(td3_mean.index, td3_mean - td3_std, td3_mean + td3_std,\n",
    "                alpha=0.1, color='tab:orange', label='TD3 Â±1 std')\n",
    "\n",
    "# Layout\n",
    "ax.set_title('Confronto SAC vs PPO vs TD3 (multi-seed, 1500 episodi)')\n",
    "ax.set_xlabel('Episodio')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "NEW REWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tutorial_3-8-20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
